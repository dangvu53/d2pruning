{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd4481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: dill in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (0.32.4)\n",
      "Requirement already satisfied: packaging in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in d:\\research\\d2pruning\\env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\research\\d2pruning\\env\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\research\\d2pruning\\env\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\research\\d2pruning\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "# Install evaluate library for load_metric function\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ad982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in d:\\research\\d2pruning\\env\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\research\\d2pruning\\env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\research\\d2pruning\\env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf0b40",
   "metadata": {},
   "source": [
    "Run on ANLI-2k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/electra-small-discriminator\"\n",
    "\"FacebookAI/roberta-base\"\n",
    "\"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb280e3",
   "metadata": {},
   "source": [
    "Train on full to get training dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a71668ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/functools.py\", line 952, in __set_name__\n",
      "    def __set_name__(self, owner, name):\n",
      "KeyboardInterrupt\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/automl/d2pruning/train_nlp_explore.py\", line 30, in <module>\n",
      "    import datasets\n",
      "  File \"/home/automl/d2pruning/venv/lib/python3.10/site-packages/datasets/__init__.py\", line 17, in <module>\n",
      "    from .arrow_dataset import Dataset\n",
      "  File \"/home/automl/d2pruning/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 54, in <module>\n",
      "    import fsspec\n",
      "  File \"/home/automl/d2pruning/venv/lib/python3.10/site-packages/fsspec/__init__.py\", line 9, in <module>\n",
      "    from .mapping import FSMap, get_mapper\n",
      "  File \"/home/automl/d2pruning/venv/lib/python3.10/site-packages/fsspec/mapping.py\", line 13, in <module>\n",
      "    class FSMap(MutableMapping):\n",
      "  File \"/usr/lib/python3.10/abc.py\", line 106, in __new__\n",
      "    cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n",
      "RuntimeError: Error calling __set_name__ on 'cached_property' instance 'dirfs' in 'FSMap'\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"FacebookAI/roberta-base\" \\\n",
    "    --output_dir ./data-model/anli-2k/all-data \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38710cc",
   "metadata": {},
   "source": [
    "Save feature embeddings and importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6aec4363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n",
      "06/28/2025 09:49:40 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 2000 to 1801 for creating validation set\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/anli-2k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 1801/1801 [00:00<00:00, 5195.64 examples/s\n",
      "Running tokenizer on dataset: 100%|██| 199/199 [00:00<00:00, 3679.86 examples/s]\n",
      "06/28/2025 09:49:50 - INFO - __main__ - Sample 1309 of the training set: {'input_ids': [101, 1996, 17324, 2080, 2056, 2200, 4415, 2008, 2065, 2017, 2020, 2000, 11092, 1996, 2343, 1005, 1055, 2933, 1010, 2045, 2052, 2022, 1037, 1002, 1016, 23458, 4920, 1999, 2591, 3036, 1010, 2138, 2651, 1005, 1055, 3667, 3477, 1999, 2000, 1996, 2291, 2005, 2651, 1005, 1055, 11036, 2229, 1012, 1998, 1996, 17324, 2080, 2056, 1011, 1011, 2008, 1005, 1055, 1996, 7740, 5166, 2436, 1025, 2009, 1005, 1055, 12170, 26053, 1011, 1011, 2027, 2056, 2008, 2045, 2052, 2031, 2000, 2022, 1037, 3013, 1999, 6666, 1997, 2423, 3867, 2000, 2871, 3867, 1012, 102, 1996, 2591, 3036, 2291, 2003, 6787, 2011, 2783, 3667, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "06/28/2025 09:49:50 - INFO - __main__ - Sample 228 of the training set: {'input_ids': [101, 5545, 8527, 1010, 21479, 8527, 1006, 1039, 1012, 16333, 2620, 1516, 1039, 1012, 14168, 2487, 1013, 4720, 1007, 2001, 3203, 21208, 7971, 2000, 2888, 9937, 1005, 1055, 2336, 1024, 4615, 2984, 1010, 4615, 3870, 1010, 2888, 21870, 1998, 3159, 3487, 1012, 1996, 2597, 1997, 3203, 21208, 7971, 1999, 2014, 2154, 15881, 2625, 2008, 1997, 1996, 2759, 2715, 2801, 1997, 1037, 21208, 7971, 1010, 2062, 2008, 1997, 1037, 19174, 1012, 102, 5545, 8527, 4669, 2014, 6139, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
      "06/28/2025 09:49:50 - INFO - __main__ - Sample 51 of the training set: {'input_ids': [101, 1037, 2611, 2171, 11531, 1026, 7987, 1028, 5199, 2001, 2356, 2000, 2265, 1996, 2047, 3076, 2105, 1012, 2002, 2001, 2000, 3524, 1999, 1996, 2082, 2436, 2005, 1037, 3076, 2315, 11531, 1012, 3403, 2005, 1996, 3076, 2000, 7180, 2002, 4999, 2054, 2027, 2052, 2022, 2066, 1012, 5199, 5071, 1996, 2711, 2052, 2022, 4206, 2066, 2032, 1998, 1037, 2879, 1012, 2043, 1996, 2711, 2633, 3369, 2009, 2001, 2460, 2611, 5102, 2035, 1999, 2630, 1012, 102, 1996, 2047, 3076, 2001, 2025, 2137, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
      "06/28/2025 09:49:51 - INFO - __main__ - ***** Running evaluation *****\n",
      "06/28/2025 09:49:51 - INFO - __main__ -   Num examples = 1801\n",
      "06/28/2025 09:49:51 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 09:49:51 - INFO - __main__ -   Total train batch size (w. parallel & distributed) = 16\n",
      "0it [00:00, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "113it [00:07, 14.50it/s]\n",
      "06/28/2025 09:49:59 - INFO - __main__ - result: {'accuracy': 0.3192670738478623}\n",
      "06/28/2025 09:49:59 - INFO - __main__ - Saved confidence values for 1801 samples at ./data-model/anli-2k/all-data/train_confs.npy\n",
      "06/28/2025 09:49:59 - INFO - __main__ - Saved feature embeddings for 1801 samples at ./data-model/anli-2k/all-data/train-features.npy\n",
      "0\n",
      "0\n",
      "Calculating LOF importance scores...\n",
      "LOF importance scores calculated and stored.\n",
      "Saving data score at ./data-model/anli-2k/all-data/data-score-goddawg.pickle\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/anli-2k/all-data \\\n",
    "    --do_eval \\\n",
    "    --eval_train \\\n",
    "    --save_feature \\\n",
    "    --save_confidence \\\n",
    "    --save_importance_scores \\\n",
    "    --training_dynamics \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288b5c3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "Prun and train on 50% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "459148a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n",
      "06/28/2025 09:50:42 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 2000 to 1801 for creating validation set\n",
      "Using descending order\n",
      "Bad data -> High priority accumulated_margin: tensor([-1.4754, -1.4671, -1.4604, -1.3715, -1.3330, -1.3192, -1.3108, -1.3044,\n",
      "        -1.2968, -1.2751, -1.2328, -1.2296, -1.2239, -1.2204, -1.2138])\n",
      "Prune 720 samples.\n",
      "Frequency of bin counts [(344, 1), (707, 1), (30, 1)]\n",
      "Skipping 0 empty bins in total 3 bins\n",
      "/home/automl/d2pruning/core/data/Coreset.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rst[sorted_index] = torch.tensor(budgets).type(torch.int)\n",
      "Using density sampling...\n",
      "Starting process for label   0 with 344 samples\n",
      "3440 connected nodes\n",
      "Starting process for label   1 with 707 samples\n",
      "7070 connected nodes\n",
      "Starting process for label   2 with 30 samples\n",
      "300 connected nodes\n",
      "06/28/2025 09:50:49 - INFO - __main__ - Pruned dataset from 1801 samples to 900 samples, retaining 0.5 of the dataset.\n",
      "['uid', 'premise', 'hypothesis', 'label', 'reason'] ['uid', 'premise', 'hypothesis', 'label', 'reason']\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
      "        num_rows: 199\n",
      "    })\n",
      "})\n",
      "Pruned 1801 samples in original train set to 900\n",
      "/home/automl/d2pruning/train_nlp_explore.py:899: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  np.save(coreset_index_path, np.array(coreset_index))\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/anli-2k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|██| 900/900 [00:00<00:00, 5447.26 examples/s]\n",
      "Running tokenizer on dataset: 100%|██| 199/199 [00:00<00:00, 4193.76 examples/s]\n",
      "06/28/2025 09:50:52 - INFO - __main__ - Sample 480 of the training set: {'input_ids': [101, 2426, 1996, 2538, 5347, 2187, 2058, 2044, 1996, 3025, 2602, 2461, 3092, 2006, 2254, 1015, 1010, 2294, 1010, 1996, 9353, 23248, 1998, 1996, 9815, 23005, 2609, 1997, 9610, 8661, 2009, 4143, 2020, 2725, 2092, 1999, 3041, 10385, 1012, 1037, 3764, 26760, 20778, 2005, 1996, 3192, 2056, 3041, 2023, 3204, 2008, 1996, 2765, 2003, 2898, 2330, 1012, 1996, 2602, 5103, 2097, 2022, 2218, 1999, 11929, 1010, 5978, 1012, 5365, 3883, 22744, 10677, 2243, 1010, 2329, 3364, 2909, 3841, 22819, 1998, 16046, 3883, 12170, 19707, 3270, 19021, 2226, 2031, 2042, 2623, 2004, 6184, 1010, 19748, 6606, 9143, 2097, 2022, 2556, 1010, 1998, 9567, 2097, 2421, 7673, 8685, 1998, 15775, 2912, 4967, 1012, 102, 22744, 10677, 2243, 2003, 2619, 2007, 20381, 1010, 1998, 2027, 2024, 26236, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "06/28/2025 09:50:52 - INFO - __main__ - Sample 490 of the training set: {'input_ids': [101, 2065, 2017, 2031, 1054, 5369, 25360, 2540, 4295, 1010, 2152, 2668, 3778, 1010, 2030, 14671, 1010, 2030, 2202, 3617, 2483, 5850, 1010, 17663, 1041, 2064, 2022, 17631, 1012, 102, 17663, 1041, 2064, 2022, 7386, 10258, 2065, 2017, 2031, 13764, 20960, 5850, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "06/28/2025 09:50:52 - INFO - __main__ - Sample 451 of the training set: {'input_ids': [101, 2129, 2000, 4374, 8404, 1026, 7987, 1028, 4521, 1037, 2740, 3771, 8738, 1012, 2913, 2265, 2008, 2111, 2040, 4521, 1037, 1000, 3671, 1000, 2137, 8738, 1997, 15514, 1998, 18015, 2833, 9015, 2013, 2062, 6245, 1010, 10089, 1010, 6888, 18755, 1010, 1998, 23760, 18908, 7730, 1012, 3046, 2000, 4521, 2740, 3771, 2065, 2017, 2215, 2000, 12992, 2115, 6888, 1998, 2444, 1037, 19366, 2166, 1012, 102, 18108, 2024, 2025, 2641, 7965, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
      "06/28/2025 09:50:53 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 09:50:53 - INFO - __main__ -   Num examples = 900\n",
      "06/28/2025 09:50:53 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 09:50:53 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
      "06/28/2025 09:50:53 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "06/28/2025 09:50:53 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 09:50:53 - INFO - __main__ -   Total optimization steps = 290\n",
      "  0%|                                                   | 0/290 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████▏                                     | 29/290 [00:10<01:34,  2.77it/s]06/28/2025 09:51:05 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.3869346733668342}\n",
      "epoch 0: validation: {'accuracy': 0.3869346733668342}\n",
      "06/28/2025 09:51:09 - INFO - __main__ - epoch 0: test: {'accuracy': 0.326}\n",
      "epoch 0: test: {'accuracy': 0.326}\n",
      " 20%|████████▍                                 | 58/290 [00:25<01:25,  2.73it/s]06/28/2025 09:51:20 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 1: validation: {'accuracy': 0.39195979899497485}\n",
      "06/28/2025 09:51:24 - INFO - __main__ - epoch 1: test: {'accuracy': 0.325}\n",
      "epoch 1: test: {'accuracy': 0.325}\n",
      " 30%|████████████▌                             | 87/290 [00:40<01:18,  2.58it/s]06/28/2025 09:51:35 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.44221105527638194}\n",
      "epoch 2: validation: {'accuracy': 0.44221105527638194}\n",
      "06/28/2025 09:51:39 - INFO - __main__ - epoch 2: test: {'accuracy': 0.32}\n",
      "epoch 2: test: {'accuracy': 0.32}\n",
      " 40%|████████████████▍                        | 116/290 [00:56<01:09,  2.52it/s]06/28/2025 09:51:51 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.45226130653266333}\n",
      "epoch 3: validation: {'accuracy': 0.45226130653266333}\n",
      "06/28/2025 09:51:55 - INFO - __main__ - epoch 3: test: {'accuracy': 0.321}\n",
      "epoch 3: test: {'accuracy': 0.321}\n",
      " 50%|████████████████████▌                    | 145/290 [01:12<00:58,  2.49it/s]06/28/2025 09:52:07 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.49246231155778897}\n",
      "epoch 4: validation: {'accuracy': 0.49246231155778897}\n",
      "06/28/2025 09:52:11 - INFO - __main__ - epoch 4: test: {'accuracy': 0.336}\n",
      "epoch 4: test: {'accuracy': 0.336}\n",
      " 60%|████████████████████████▌                | 174/290 [01:28<00:43,  2.69it/s]06/28/2025 09:52:23 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.46733668341708545}\n",
      "epoch 5: validation: {'accuracy': 0.46733668341708545}\n",
      " 70%|████████████████████████████▋            | 203/290 [01:40<00:32,  2.70it/s]06/28/2025 09:52:34 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.49246231155778897}\n",
      "epoch 6: validation: {'accuracy': 0.49246231155778897}\n",
      "06/28/2025 09:52:39 - INFO - __main__ - epoch 6: test: {'accuracy': 0.346}\n",
      "epoch 6: test: {'accuracy': 0.346}\n",
      " 80%|████████████████████████████████▊        | 232/290 [01:56<00:22,  2.59it/s]06/28/2025 09:52:50 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.49748743718592964}\n",
      "epoch 7: validation: {'accuracy': 0.49748743718592964}\n",
      "06/28/2025 09:52:55 - INFO - __main__ - epoch 7: test: {'accuracy': 0.336}\n",
      "epoch 7: test: {'accuracy': 0.336}\n",
      " 90%|████████████████████████████████████▉    | 261/290 [02:12<00:11,  2.60it/s]06/28/2025 09:53:07 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.5025125628140703}\n",
      "epoch 8: validation: {'accuracy': 0.5025125628140703}\n",
      "06/28/2025 09:53:11 - INFO - __main__ - epoch 8: test: {'accuracy': 0.342}\n",
      "epoch 8: test: {'accuracy': 0.342}\n",
      "100%|█████████████████████████████████████████| 290/290 [02:28<00:00,  2.51it/s]06/28/2025 09:53:23 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.5125628140703518}\n",
      "epoch 9: validation: {'accuracy': 0.5125628140703518}\n",
      "06/28/2025 09:53:27 - INFO - __main__ - epoch 9: test: {'accuracy': 0.338}\n",
      "epoch 9: test: {'accuracy': 0.338}\n",
      "100%|█████████████████████████████████████████| 290/290 [02:33<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "CORESET_RATIO = 0.5\n",
    "N_NEIGHBOR = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/anli-2k/coreset-0.5 \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --coreset \\\n",
    "    --coreset-mode class \\\n",
    "    --budget-mode uniform \\\n",
    "    --sampling-mode graph \\\n",
    "    --data-score-path ./data-model/anli-2k/all-data/data-score-goddawg.pickle \\\n",
    "    --feature-path ./data-model/anli-2k/all-data/train-features.npy \\\n",
    "    --coreset-key forgetting \\\n",
    "    --coreset-ratio {CORESET_RATIO} \\\n",
    "    --mis-ratio 0.4 \\\n",
    "    --label-balanced \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --n-neighbor {N_NEIGHBOR} \\\n",
    "    --gamma {GAMMA} \\\n",
    "    --graph-mode sum \\\n",
    "    --graph-sampling-mode weighted \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888302e0",
   "metadata": {},
   "source": [
    "Train D(Full data) and D'(50% coreset) on BERT base uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ff245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path bert-base-uncased \\\n",
    "    --output_dir ./data-model/anli-2k/roberta_base/bert/ \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fda05b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n",
      "06/28/2025 10:00:36 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 2000 to 1801 for creating validation set\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/anli-2k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|██| 900/900 [00:00<00:00, 5434.55 examples/s]\n",
      "Running tokenizer on dataset: 100%|██| 199/199 [00:00<00:00, 4483.84 examples/s]\n",
      "06/28/2025 10:00:45 - INFO - __main__ - Sample 654 of the training set: (654, {'input_ids': [101, 5626, 1006, 1059, 18037, 2480, 1007, 1011, 2610, 2024, 11538, 2044, 2028, 2158, 2001, 2730, 1998, 2178, 2001, 5229, 2012, 1037, 3902, 2644, 5008, 4465, 2305, 1010, 2610, 2056, 1012, 2009, 3047, 2012, 2055, 1021, 1024, 2382, 1052, 1012, 1049, 1012, 2379, 11290, 3630, 2483, 1998, 6031, 2006, 5626, 1005, 1055, 2225, 2217, 1012, 2610, 2056, 1996, 2048, 2273, 2024, 1999, 2037, 2322, 1005, 1055, 1012, 1996, 6405, 6778, 2165, 1037, 3902, 2000, 1996, 2902, 1010, 2073, 2002, 2003, 3205, 1999, 6540, 4650, 1010, 2610, 2794, 1012, 2610, 2031, 2053, 2592, 2055, 1037, 8343, 2012, 2023, 2051, 1012, 2065, 2017, 2031, 2151, 2592, 2055, 2023, 5008, 1010, 2655, 5626, 2610, 1012, 2994, 2007, 1059, 18037, 2480, 1012, 4012, 2005, 14409, 2006, 2023, 4975, 2466, 1012, 102, 1037, 2158, 1999, 2010, 2322, 1005, 1055, 2001, 2730, 1999, 1037, 5626, 3902, 2644, 5008, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "06/28/2025 10:00:45 - INFO - __main__ - Sample 114 of the training set: (114, {'input_ids': [101, 2067, 25046, 2188, 2932, 2003, 1037, 12170, 1011, 7058, 2137, 2932, 1012, 2009, 2001, 2631, 1999, 2960, 1999, 1037, 7381, 1999, 21151, 1010, 2662, 1010, 2011, 4913, 17971, 1998, 2010, 2698, 1011, 2095, 1011, 2214, 2684, 1010, 8194, 1012, 2044, 4772, 1997, 1996, 2117, 3277, 1010, 17971, 2777, 17869, 2638, 13854, 1010, 2040, 2150, 2010, 2564, 1998, 4256, 1011, 1999, 1011, 4772, 1012, 1000, 2067, 25046, 2188, 1000, 3464, 2155, 1011, 3079, 1012, 2009, 2003, 2241, 1999, 2751, 3509, 1010, 5392, 1012, 102, 1996, 2932, 2003, 2405, 12170, 7058, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "06/28/2025 10:00:45 - INFO - __main__ - Sample 25 of the training set: (25, {'input_ids': [101, 2059, 1045, 2404, 2006, 2014, 2197, 3467, 1005, 1055, 2283, 4377, 1012, 1026, 7987, 1028, 2009, 2001, 2107, 1037, 3492, 5122, 3756, 2518, 1010, 2007, 12817, 1997, 2304, 12922, 1010, 1998, 2009, 2106, 1050, 1005, 1056, 3043, 2055, 2049, 2108, 1037, 2210, 2214, 1011, 13405, 1010, 2144, 2009, 7130, 2033, 2066, 1037, 15913, 1012, 1026, 7987, 1028, 2633, 1045, 3706, 2067, 1998, 2246, 2012, 2870, 1012, 1026, 7987, 1028, 1045, 2387, 1037, 2450, 1999, 2008, 3221, 1012, 1012, 1012, 1037, 4206, 1010, 3442, 6492, 2007, 11466, 6029, 1998, 10156, 2159, 1012, 1012, 1012, 1998, 1996, 2245, 1999, 2026, 2568, 2001, 2061, 29204, 2008, 2009, 2056, 2993, 12575, 1024, 1036, 1036, 2821, 1010, 1045, 4299, 11430, 2071, 2156, 2033, 2085, 999, 1005, 1005, 1026, 7987, 1028, 2012, 2008, 2200, 2617, 1996, 10850, 6573, 2012, 1996, 2341, 2000, 2425, 2033, 2008, 2720, 1012, 11759, 2001, 10025, 4851, 2005, 2033, 1012, 102, 1996, 4377, 6272, 2000, 2014, 2905, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}).\n",
      "06/28/2025 10:00:47 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 10:00:47 - INFO - __main__ -   Num examples = 900\n",
      "06/28/2025 10:00:47 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 10:00:47 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 10:00:47 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "06/28/2025 10:00:47 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 10:00:47 - INFO - __main__ -   Total optimization steps = 570\n",
      "  0%|                                                   | 0/570 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████▏                                     | 57/570 [00:11<01:31,  5.58it/s]06/28/2025 10:00:59 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.3869346733668342}\n",
      "epoch 0: validation: {'accuracy': 0.3869346733668342}\n",
      "06/28/2025 10:01:03 - INFO - __main__ - epoch 0: test: {'accuracy': 0.327}\n",
      "epoch 0: test: {'accuracy': 0.327}\n",
      " 20%|████████▏                                | 114/570 [00:26<01:21,  5.63it/s]06/28/2025 10:01:14 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.4120603015075377}\n",
      "epoch 1: validation: {'accuracy': 0.4120603015075377}\n",
      "06/28/2025 10:01:18 - INFO - __main__ - epoch 1: test: {'accuracy': 0.315}\n",
      "epoch 1: test: {'accuracy': 0.315}\n",
      " 30%|████████████▎                            | 171/570 [00:42<01:16,  5.21it/s]06/28/2025 10:01:30 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.457286432160804}\n",
      "epoch 2: validation: {'accuracy': 0.457286432160804}\n",
      "06/28/2025 10:01:34 - INFO - __main__ - epoch 2: test: {'accuracy': 0.329}\n",
      "epoch 2: test: {'accuracy': 0.329}\n",
      " 40%|████████████████▍                        | 228/570 [00:58<01:03,  5.36it/s]06/28/2025 10:01:46 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.48743718592964824}\n",
      "epoch 3: validation: {'accuracy': 0.48743718592964824}\n",
      "06/28/2025 10:01:50 - INFO - __main__ - epoch 3: test: {'accuracy': 0.339}\n",
      "epoch 3: test: {'accuracy': 0.339}\n",
      " 50%|████████████████████▌                    | 285/570 [01:14<00:56,  5.03it/s]06/28/2025 10:02:02 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.49748743718592964}\n",
      "epoch 4: validation: {'accuracy': 0.49748743718592964}\n",
      "06/28/2025 10:02:06 - INFO - __main__ - epoch 4: test: {'accuracy': 0.343}\n",
      "epoch 4: test: {'accuracy': 0.343}\n",
      " 60%|████████████████████████▌                | 342/570 [01:30<00:39,  5.74it/s]06/28/2025 10:02:18 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.5025125628140703}\n",
      "epoch 5: validation: {'accuracy': 0.5025125628140703}\n",
      "06/28/2025 10:02:23 - INFO - __main__ - epoch 5: test: {'accuracy': 0.341}\n",
      "epoch 5: test: {'accuracy': 0.341}\n",
      " 70%|████████████████████████████▋            | 399/570 [01:46<00:31,  5.45it/s]06/28/2025 10:02:35 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.49246231155778897}\n",
      "epoch 6: validation: {'accuracy': 0.49246231155778897}\n",
      " 80%|████████████████████████████████▊        | 456/570 [01:58<00:20,  5.60it/s]06/28/2025 10:02:47 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.49246231155778897}\n",
      "epoch 7: validation: {'accuracy': 0.49246231155778897}\n",
      " 90%|████████████████████████████████████▉    | 513/570 [02:10<00:10,  5.24it/s]06/28/2025 10:02:58 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.47738693467336685}\n",
      "epoch 8: validation: {'accuracy': 0.47738693467336685}\n",
      "100%|█████████████████████████████████████████| 570/570 [02:22<00:00,  5.34it/s]06/28/2025 10:03:11 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.4824120603015075}\n",
      "epoch 9: validation: {'accuracy': 0.4824120603015075}\n",
      "100%|█████████████████████████████████████████| 570/570 [02:23<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/anli-2k/bert_base/bert-50%/ \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --train-index-path ./data-model/anli-2k/coreset-0.5/coreset-goddawg.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb34aec",
   "metadata": {},
   "source": [
    "### Run on AGNews-5K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31713022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google-bert/bert-base-uncased'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"google/electra-small-discriminator\"\n",
    "\"FacebookAI/roberta-base\"\n",
    "\"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44be368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n",
      "06/28/2025 18:41:15 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 5000/5000 [00:00<00:00, 7799.81 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 7597.29 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8008.06 examples/s\n",
      "06/28/2025 18:41:24 - INFO - __main__ - Sample 912 of the training set: (912, {'input_ids': [101, 3956, 1005, 1055, 10666, 3210, 2039, 2490, 2005, 5622, 5283, 2094, 3789, 102, 6744, 1006, 26665, 1007, 1011, 5611, 3539, 2704, 16126, 10666, 8610, 2098, 2490, 2005, 9432, 1005, 1055, 5622, 5283, 2094, 2283, 3789, 2000, 3288, 1996, 4450, 2283, 2046, 1037, 2231, 2008, 2052, 3796, 10245, 3864, 1998, 6643, 3726, 1996, 2126, 2005, 1037, 14474, 10534, 2279, 2095, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "06/28/2025 18:41:24 - INFO - __main__ - Sample 204 of the training set: (204, {'input_ids': [101, 13420, 6971, 2039, 2007, 25353, 14905, 2937, 102, 2044, 3773, 2210, 3112, 2007, 1996, 4431, 5617, 2009, 2580, 2005, 7513, 1001, 4464, 1025, 1055, 26381, 4132, 1010, 13420, 2097, 2085, 2147, 2007, 25353, 14905, 2937, 2000, 3443, 4431, 7248, 2005, 1017, 2290, 2398, 8454, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "06/28/2025 18:41:24 - INFO - __main__ - Sample 2253 of the training set: (2253, {'input_ids': [101, 4654, 3540, 22879, 5668, 7523, 2322, 12954, 28397, 1999, 5279, 1006, 9706, 1007, 102, 9706, 1011, 4654, 3540, 22879, 5668, 3603, 2322, 23880, 12954, 28397, 1999, 1996, 13253, 3089, 3148, 18128, 1999, 2530, 5279, 1010, 1996, 2231, 1005, 1055, 2473, 1997, 21387, 2056, 9857, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "06/28/2025 18:41:26 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Num examples = 5000\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Total optimization steps = 3130\n",
      "  0%|                                                  | 0/3130 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|███▉                                    | 312/3130 [00:42<06:30,  7.22it/s]06/28/2025 18:42:12 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.896}\n",
      "epoch 0: validation: {'accuracy': 0.896}\n",
      "06/28/2025 18:42:15 - INFO - __main__ - epoch 0: test: {'accuracy': 0.906}\n",
      "epoch 0: test: {'accuracy': 0.906}\n",
      " 20%|███████▉                                | 625/3130 [01:33<05:22,  7.77it/s]06/28/2025 18:43:02 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.895}\n",
      "epoch 1: validation: {'accuracy': 0.895}\n",
      " 30%|████████████                            | 939/3130 [02:21<04:25,  8.24it/s]06/28/2025 18:43:50 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.908}\n",
      "epoch 2: validation: {'accuracy': 0.908}\n",
      "06/28/2025 18:43:53 - INFO - __main__ - epoch 2: test: {'accuracy': 0.913}\n",
      "epoch 2: test: {'accuracy': 0.913}\n",
      " 40%|███████████████▌                       | 1252/3130 [03:12<04:33,  6.87it/s]06/28/2025 18:44:41 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.904}\n",
      "epoch 3: validation: {'accuracy': 0.904}\n",
      " 50%|███████████████████▌                   | 1565/3130 [04:00<03:32,  7.38it/s]06/28/2025 18:45:29 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.905}\n",
      "epoch 4: validation: {'accuracy': 0.905}\n",
      " 60%|███████████████████████▍               | 1878/3130 [04:48<03:33,  5.86it/s]06/28/2025 18:46:18 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.914}\n",
      "epoch 5: validation: {'accuracy': 0.914}\n",
      "06/28/2025 18:46:21 - INFO - __main__ - epoch 5: test: {'accuracy': 0.911}\n",
      "epoch 5: test: {'accuracy': 0.911}\n",
      " 70%|███████████████████████████▎           | 2190/3130 [05:39<01:55,  8.17it/s]06/28/2025 18:47:08 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.903}\n",
      "epoch 6: validation: {'accuracy': 0.903}\n",
      " 80%|███████████████████████████████▏       | 2504/3130 [06:27<01:27,  7.14it/s]06/28/2025 18:47:56 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.907}\n",
      "epoch 7: validation: {'accuracy': 0.907}\n",
      " 90%|███████████████████████████████████    | 2817/3130 [07:14<00:36,  8.61it/s]06/28/2025 18:48:44 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.905}\n",
      "epoch 8: validation: {'accuracy': 0.905}\n",
      "100%|███████████████████████████████████████| 3130/3130 [08:02<00:00,  7.97it/s]06/28/2025 18:49:32 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.907}\n",
      "epoch 9: validation: {'accuracy': 0.907}\n",
      "100%|███████████████████████████████████████| 3130/3130 [08:06<00:00,  6.43it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/agnews-6k/all-data \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k//val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n",
      "06/28/2025 18:39:30 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 5000/5000 [00:00<00:00, 7755.22 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8042.38 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8217.34 examples/s\n",
      "06/28/2025 18:39:38 - INFO - __main__ - Sample 912 of the training set: {'input_ids': [101, 3956, 1005, 1055, 10666, 3210, 2039, 2490, 2005, 5622, 5283, 2094, 3789, 102, 6744, 1006, 26665, 1007, 1011, 5611, 3539, 2704, 16126, 10666, 8610, 2098, 2490, 2005, 9432, 1005, 1055, 5622, 5283, 2094, 2283, 3789, 2000, 3288, 1996, 4450, 2283, 2046, 1037, 2231, 2008, 2052, 3796, 10245, 3864, 1998, 6643, 3726, 1996, 2126, 2005, 1037, 14474, 10534, 2279, 2095, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "06/28/2025 18:39:38 - INFO - __main__ - Sample 204 of the training set: {'input_ids': [101, 13420, 6971, 2039, 2007, 25353, 14905, 2937, 102, 2044, 3773, 2210, 3112, 2007, 1996, 4431, 5617, 2009, 2580, 2005, 7513, 1001, 4464, 1025, 1055, 26381, 4132, 1010, 13420, 2097, 2085, 2147, 2007, 25353, 14905, 2937, 2000, 3443, 4431, 7248, 2005, 1017, 2290, 2398, 8454, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}.\n",
      "06/28/2025 18:39:38 - INFO - __main__ - Sample 2253 of the training set: {'input_ids': [101, 4654, 3540, 22879, 5668, 7523, 2322, 12954, 28397, 1999, 5279, 1006, 9706, 1007, 102, 9706, 1011, 4654, 3540, 22879, 5668, 3603, 2322, 23880, 12954, 28397, 1999, 1996, 13253, 3089, 3148, 18128, 1999, 2530, 5279, 1010, 1996, 2231, 1005, 1055, 2473, 1997, 21387, 2056, 9857, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}.\n",
      "06/28/2025 18:39:40 - INFO - __main__ - ***** Running evaluation *****\n",
      "06/28/2025 18:39:40 - INFO - __main__ -   Num examples = 5000\n",
      "06/28/2025 18:39:40 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 18:39:40 - INFO - __main__ -   Total train batch size (w. parallel & distributed) = 16\n",
      "0it [00:00, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "313it [00:10, 29.80it/s]\n",
      "06/28/2025 18:39:50 - INFO - __main__ - result: {'accuracy': 0.2532}\n",
      "06/28/2025 18:39:50 - INFO - __main__ - Saved confidence values for 5000 samples at ./data-model/agnews-6k//all-data/train_confs.npy\n",
      "06/28/2025 18:39:50 - INFO - __main__ - Saved feature embeddings for 5000 samples at ./data-model/agnews-6k//all-data/train-features.npy\n",
      "0\n",
      "0\n",
      "Calculating LOF importance scores...\n",
      "LOF importance scores calculated and stored.\n",
      "Saving data score at ./data-model/agnews-6k//all-data/data-score-goddawg.pickle\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"google/electra-small-discriminator\"\\\n",
    "    --output_dir ./data-model/agnews-6k//all-data \\\n",
    "    --do_eval \\\n",
    "    --eval_train \\\n",
    "    --save_feature \\\n",
    "    --save_confidence \\\n",
    "    --save_importance_scores \\\n",
    "    --training_dynamics \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k/val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aaddc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/28/2025 17:40:17 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "Using descending order\n",
      "Bad data -> High priority accumulated_margin: tensor([-9.8320, -8.8350, -8.3520, -6.6949, -6.4507, -6.0567, -5.7893, -5.6004,\n",
      "        -5.4415, -5.3974, -3.9379, -3.7875, -3.3515, -3.2190, -3.0092])\n",
      "Prune 2000 samples.\n",
      "Frequency of bin counts [(838, 1), (928, 1), (643, 1), (591, 1)]\n",
      "Skipping 0 empty bins in total 4 bins\n",
      "/home/automl/d2pruning/core/data/Coreset.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rst[sorted_index] = torch.tensor(budgets).type(torch.int)\n",
      "Using density sampling...\n",
      "Starting process for label   0 with 838 samples\n",
      "8380 connected nodes\n",
      "Starting process for label   1 with 928 samples\n",
      "9280 connected nodes\n",
      "Starting process for label   2 with 643 samples\n",
      "6430 connected nodes\n",
      "Starting process for label   3 with 591 samples\n",
      "5910 connected nodes\n",
      "06/28/2025 17:40:25 - INFO - __main__ - Pruned dataset from 5000 samples to 2500 samples, retaining 0.5 of the dataset.\n",
      "['label', 'title', 'description'] ['label', 'title', 'description']\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'title', 'description'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'description'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'title', 'description'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "Pruned 5000 samples in original train set to 2500\n",
      "/home/automl/d2pruning/train_nlp_explore.py:899: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  np.save(coreset_index_path, np.array(coreset_index))\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 2500/2500 [00:00<00:00, 7249.11 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 7955.78 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8054.89 examples/s\n",
      "06/28/2025 17:40:26 - INFO - __main__ - Sample 1372 of the training set: {'input_ids': [101, 12175, 2000, 13210, 7315, 3084, 7541, 3413, 1999, 3515, 2487, 2086, 1006, 26665, 1007, 102, 26665, 1011, 2019, 12175, 2315, 2005, 1037, 8730, 2643, 1032, 1997, 2162, 2097, 2272, 2004, 2485, 2000, 3011, 2023, 2733, 2004, 2009, 2038, 2144, 1032, 11502, 2509, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}.\n",
      "06/28/2025 17:40:26 - INFO - __main__ - Sample 1165 of the training set: {'input_ids': [101, 9098, 5016, 7151, 1053, 2509, 3463, 15488, 23212, 2078, 1005, 102, 2414, 1006, 26665, 1007, 1011, 2329, 2137, 9098, 1996, 2088, 1005, 1055, 2117, 5221, 9907, 9338, 1010, 3786, 19939, 2015, 2007, 1037, 2410, 1012, 1019, 3867, 4125, 1999, 2353, 1011, 4284, 11372, 2004, 27999, 6202, 2013, 3304, 1998, 3607, 16396, 3010, 3471, 1998, 1996, 2844, 9044, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.\n",
      "06/28/2025 17:40:26 - INFO - __main__ - Sample 1861 of the training set: {'input_ids': [101, 2149, 5268, 4152, 2423, 2086, 1999, 4028, 1997, 8956, 3457, 102, 1037, 2149, 5268, 2001, 7331, 2000, 2423, 2086, 1999, 3827, 2005, 1996, 4028, 1997, 2019, 8956, 2120, 4932, 2386, 1999, 2089, 1010, 1996, 2149, 2510, 2056, 5095, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "06/28/2025 17:40:28 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Num examples = 2500\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Total optimization steps = 790\n",
      "  0%|                                                   | 0/790 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████▏                                     | 79/790 [00:07<00:58, 12.17it/s]06/28/2025 17:40:39 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.814}\n",
      "epoch 0: validation: {'accuracy': 0.814}\n",
      "06/28/2025 17:40:42 - INFO - __main__ - epoch 0: test: {'accuracy': 0.818}\n",
      "epoch 0: test: {'accuracy': 0.818}\n",
      " 20%|████████▏                                | 157/790 [00:20<00:50, 12.49it/s]06/28/2025 17:40:51 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.859}\n",
      "epoch 1: validation: {'accuracy': 0.859}\n",
      "06/28/2025 17:40:54 - INFO - __main__ - epoch 1: test: {'accuracy': 0.883}\n",
      "epoch 1: test: {'accuracy': 0.883}\n",
      " 30%|████████████▎                            | 237/790 [00:32<00:44, 12.33it/s]06/28/2025 17:41:04 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.863}\n",
      "epoch 2: validation: {'accuracy': 0.863}\n",
      "06/28/2025 17:41:06 - INFO - __main__ - epoch 2: test: {'accuracy': 0.885}\n",
      "epoch 2: test: {'accuracy': 0.885}\n",
      " 40%|████████████████▎                        | 315/790 [00:44<00:39, 11.97it/s]06/28/2025 17:41:16 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.866}\n",
      "epoch 3: validation: {'accuracy': 0.866}\n",
      "06/28/2025 17:41:19 - INFO - __main__ - epoch 3: test: {'accuracy': 0.889}\n",
      "epoch 3: test: {'accuracy': 0.889}\n",
      " 50%|████████████████████▌                    | 395/790 [00:57<00:31, 12.71it/s]06/28/2025 17:41:28 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.87}\n",
      "epoch 4: validation: {'accuracy': 0.87}\n",
      "06/28/2025 17:41:31 - INFO - __main__ - epoch 4: test: {'accuracy': 0.878}\n",
      "epoch 4: test: {'accuracy': 0.878}\n",
      " 60%|████████████████████████▌                | 473/790 [01:09<00:26, 12.17it/s]06/28/2025 17:41:41 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.865}\n",
      "epoch 5: validation: {'accuracy': 0.865}\n",
      " 70%|████████████████████████████▋            | 553/790 [01:19<00:19, 12.37it/s]06/28/2025 17:41:50 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.865}\n",
      "epoch 6: validation: {'accuracy': 0.865}\n",
      " 80%|████████████████████████████████▋        | 631/790 [01:28<00:13, 11.84it/s]06/28/2025 17:42:00 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.867}\n",
      "epoch 7: validation: {'accuracy': 0.867}\n",
      " 90%|████████████████████████████████████▉    | 711/790 [01:37<00:06, 12.27it/s]06/28/2025 17:42:09 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.872}\n",
      "epoch 8: validation: {'accuracy': 0.872}\n",
      "06/28/2025 17:42:12 - INFO - __main__ - epoch 8: test: {'accuracy': 0.883}\n",
      "epoch 8: test: {'accuracy': 0.883}\n",
      "100%|████████████████████████████████████████▉| 789/790 [01:50<00:00, 12.08it/s]06/28/2025 17:42:21 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.871}\n",
      "epoch 9: validation: {'accuracy': 0.871}\n",
      "100%|█████████████████████████████████████████| 790/790 [01:53<00:00,  6.97it/s]\n"
     ]
    }
   ],
   "source": [
    "CORESET_RATIO = 0.5\n",
    "N_NEIGHBOR = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"google/electra-small-discriminator\" \\\n",
    "    --output_dir ./data-model/agnews-6k//coreset-0.5 \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --coreset \\\n",
    "    --coreset-mode class \\\n",
    "    --budget-mode uniform \\\n",
    "    --sampling-mode graph \\\n",
    "    --data-score-path ./data-model/agnews-6k/all-data/data-score-goddawg.pickle \\\n",
    "    --feature-path ./data-model/agnews-6k//all-data/train-features.npy \\\n",
    "    --coreset-key forgetting \\\n",
    "    --coreset-ratio {CORESET_RATIO} \\\n",
    "    --mis-ratio 0.4 \\\n",
    "    --label-balanced \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k/val_index.npy \\\n",
    "    --n-neighbor {N_NEIGHBOR} \\\n",
    "    --gamma {GAMMA} \\\n",
    "    --graph-mode sum \\\n",
    "    --graph-sampling-mode weighted \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c8a8837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/28/2025 17:51:33 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json\n",
      "loading file merges.txt from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors\n",
      "Some weights of the model checkpoint at FacebookAI/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 2500/2500 [00:00<00:00, 8378.58 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8769.81 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 9323.37 examples/s\n",
      "06/28/2025 17:51:42 - INFO - __main__ - Sample 456 of the training set: (456, {'input_ids': [0, 791, 4, 104, 4, 4787, 42723, 15674, 6449, 29, 39286, 4995, 13303, 36, 1251, 43, 2, 2, 1251, 111, 83, 121, 4, 104, 4, 9338, 15, 395, 37457, 29639, 37, 56, 303, 5, 685, 25492, 9, 39286, 11, 5, 37457, 5412, 219, 1844, 160, 13303, 480, 1271, 39, 6680, 7, 10, 9001, 61, 37457, 7333, 33396, 38897, 13, 11505, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "06/28/2025 17:51:42 - INFO - __main__ - Sample 102 of the training set: (102, {'input_ids': [0, 7199, 241, 9038, 160, 7, 3306, 386, 2, 2, 133, 18563, 3445, 1357, 5, 191, 378, 363, 19, 10, 291, 12, 1360, 872, 23, 37802, 2880, 6, 10, 819, 14, 630, 849, 3416, 131, 90, 492, 203, 1034, 13, 10, 11713, 191, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}).\n",
      "06/28/2025 17:51:42 - INFO - __main__ - Sample 1126 of the training set: (1126, {'input_ids': [0, 19739, 42263, 9722, 37749, 7093, 3407, 1643, 11, 31589, 13623, 36, 1251, 43, 2, 2, 1251, 111, 20, 3234, 7093, 3407, 21324, 333, 381, 3847, 37457, 1193, 9725, 24, 21, 45, 10, 1240, 1370, 19, 273, 18, 13662, 37457, 25764, 23, 292, 3622, 12309, 4492, 6, 53, 24, 67, 617, 37457, 36508, 1070, 5, 11843, 14385, 19485, 108, 8099, 559, 4181, 6, 8324, 281, 4989, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "06/28/2025 17:51:44 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Num examples = 2500\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Total optimization steps = 1570\n",
      "  0%|                                                  | 0/1570 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████                                    | 157/1570 [00:22<03:15,  7.23it/s]06/28/2025 17:52:09 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.882}\n",
      "epoch 0: validation: {'accuracy': 0.882}\n",
      "06/28/2025 17:52:12 - INFO - __main__ - epoch 0: test: {'accuracy': 0.897}\n",
      "epoch 0: test: {'accuracy': 0.897}\n",
      " 20%|████████                                | 314/1570 [00:51<03:21,  6.23it/s]06/28/2025 17:52:38 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.865}\n",
      "epoch 1: validation: {'accuracy': 0.865}\n",
      " 30%|████████████                            | 471/1570 [01:17<02:39,  6.88it/s]06/28/2025 17:53:04 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.883}\n",
      "epoch 2: validation: {'accuracy': 0.883}\n",
      "06/28/2025 17:53:07 - INFO - __main__ - epoch 2: test: {'accuracy': 0.892}\n",
      "epoch 2: test: {'accuracy': 0.892}\n",
      " 40%|████████████████                        | 628/1570 [01:46<02:16,  6.91it/s]06/28/2025 17:53:33 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.88}\n",
      "epoch 3: validation: {'accuracy': 0.88}\n",
      " 50%|████████████████████                    | 785/1570 [02:12<01:42,  7.64it/s]06/28/2025 17:53:59 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.886}\n",
      "epoch 4: validation: {'accuracy': 0.886}\n",
      "06/28/2025 17:54:02 - INFO - __main__ - epoch 4: test: {'accuracy': 0.899}\n",
      "epoch 4: test: {'accuracy': 0.899}\n",
      " 60%|████████████████████████                | 942/1570 [02:41<01:27,  7.16it/s]06/28/2025 17:54:28 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.885}\n",
      "epoch 5: validation: {'accuracy': 0.885}\n",
      " 70%|███████████████████████████▎           | 1099/1570 [03:07<01:01,  7.70it/s]06/28/2025 17:54:54 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.886}\n",
      "epoch 6: validation: {'accuracy': 0.886}\n",
      "06/28/2025 17:54:57 - INFO - __main__ - epoch 6: test: {'accuracy': 0.904}\n",
      "epoch 6: test: {'accuracy': 0.904}\n",
      " 80%|███████████████████████████████▏       | 1256/1570 [03:36<00:42,  7.46it/s]06/28/2025 17:55:23 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.882}\n",
      "epoch 7: validation: {'accuracy': 0.882}\n",
      " 90%|███████████████████████████████████    | 1413/1570 [04:02<00:21,  7.14it/s]06/28/2025 17:55:49 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.882}\n",
      "epoch 8: validation: {'accuracy': 0.882}\n",
      "100%|███████████████████████████████████████| 1570/1570 [04:28<00:00,  7.18it/s]06/28/2025 17:56:15 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.881}\n",
      "epoch 9: validation: {'accuracy': 0.881}\n",
      "100%|███████████████████████████████████████| 1570/1570 [04:31<00:00,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"FacebookAI/roberta-base\" \\\n",
    "    --output_dir ./data-model/agnews-6k/electra/bert-50%/ \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k/val_index.npy \\\n",
    "    --train-index-path ./data-model/agnews-6k//coreset-0.5/coreset-goddawg.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fec2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/electra-small-discriminator\"\n",
    "\"FacebookAI/roberta-base\"\n",
    "\"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9fb3b",
   "metadata": {},
   "source": [
    "### Run on AGNews-5K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "331492c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eded826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./.venv/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading aiohttp-3.12.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.6.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [multidict]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [multidict]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [multidict]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [multidict]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [datasets]/15\u001b[0m [datasets]]ss]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 datasets-3.6.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.1 multiprocess-0.70.16 propcache-0.3.2 pyarrow-20.0.0 xxhash-3.5.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b6349d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google-bert/bert-base-uncased'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"google/electra-small-discriminator\"\n",
    "\"FacebookAI/roberta-base\"\n",
    "\"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9236ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/drive1/nammt/d2pruning\n",
      "06/28/2025 16:49:36 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "/drive1/nammt/d2pruning/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 5000/5000 [00:00<00:00, 8997.58 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8883.00 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8694.88 examples/s\n",
      "06/28/2025 16:49:45 - INFO - __main__ - Sample 912 of the training set: (912, {'input_ids': [101, 3956, 1005, 1055, 10666, 3210, 2039, 2490, 2005, 5622, 5283, 2094, 3789, 102, 6744, 1006, 26665, 1007, 1011, 5611, 3539, 2704, 16126, 10666, 8610, 2098, 2490, 2005, 9432, 1005, 1055, 5622, 5283, 2094, 2283, 3789, 2000, 3288, 1996, 4450, 2283, 2046, 1037, 2231, 2008, 2052, 3796, 10245, 3864, 1998, 6643, 3726, 1996, 2126, 2005, 1037, 14474, 10534, 2279, 2095, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "06/28/2025 16:49:45 - INFO - __main__ - Sample 204 of the training set: (204, {'input_ids': [101, 13420, 6971, 2039, 2007, 25353, 14905, 2937, 102, 2044, 3773, 2210, 3112, 2007, 1996, 4431, 5617, 2009, 2580, 2005, 7513, 1001, 4464, 1025, 1055, 26381, 4132, 1010, 13420, 2097, 2085, 2147, 2007, 25353, 14905, 2937, 2000, 3443, 4431, 7248, 2005, 1017, 2290, 2398, 8454, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "06/28/2025 16:49:45 - INFO - __main__ - Sample 2253 of the training set: (2253, {'input_ids': [101, 4654, 3540, 22879, 5668, 7523, 2322, 12954, 28397, 1999, 5279, 1006, 9706, 1007, 102, 9706, 1011, 4654, 3540, 22879, 5668, 3603, 2322, 23880, 12954, 28397, 1999, 1996, 13253, 3089, 3148, 18128, 1999, 2530, 5279, 1010, 1996, 2231, 1005, 1055, 2473, 1997, 21387, 2056, 9857, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "Downloading builder script: 4.20kB [00:00, 5.97MB/s]\n",
      "06/28/2025 16:49:48 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Num examples = 5000\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Total optimization steps = 3130\n",
      "  0%|                                                  | 0/3130 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|███▉                                    | 312/3130 [00:43<06:39,  7.05it/s]06/28/2025 16:50:35 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.896}\n",
      "epoch 0: validation: {'accuracy': 0.896}\n",
      "06/28/2025 16:50:38 - INFO - __main__ - epoch 0: test: {'accuracy': 0.906}\n",
      "epoch 0: test: {'accuracy': 0.906}\n",
      " 20%|████████                                | 626/3130 [01:34<05:07,  8.13it/s]06/28/2025 16:51:25 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.895}\n",
      "epoch 1: validation: {'accuracy': 0.895}\n",
      "06/28/2025 16:51:28 - INFO - __main__ - epoch 1: test: {'accuracy': 0.897}\n",
      "epoch 1: test: {'accuracy': 0.897}\n",
      " 30%|████████████                            | 939/3130 [02:25<04:57,  7.36it/s]06/28/2025 16:52:16 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.904}\n",
      "epoch 2: validation: {'accuracy': 0.904}\n",
      "06/28/2025 16:52:19 - INFO - __main__ - epoch 2: test: {'accuracy': 0.903}\n",
      "epoch 2: test: {'accuracy': 0.903}\n",
      " 40%|███████████████▌                       | 1252/3130 [03:17<04:42,  6.64it/s]06/28/2025 16:53:08 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.91}\n",
      "epoch 3: validation: {'accuracy': 0.91}\n",
      "06/28/2025 16:53:11 - INFO - __main__ - epoch 3: test: {'accuracy': 0.907}\n",
      "epoch 3: test: {'accuracy': 0.907}\n",
      " 50%|███████████████████▌                   | 1565/3130 [04:08<03:13,  8.08it/s]06/28/2025 16:53:59 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.905}\n",
      "epoch 4: validation: {'accuracy': 0.905}\n",
      "06/28/2025 16:54:02 - INFO - __main__ - epoch 4: test: {'accuracy': 0.916}\n",
      "epoch 4: test: {'accuracy': 0.916}\n",
      " 60%|███████████████████████▍               | 1877/3130 [05:00<03:07,  6.67it/s]06/28/2025 16:54:51 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.9}\n",
      "epoch 5: validation: {'accuracy': 0.9}\n",
      "06/28/2025 16:54:54 - INFO - __main__ - epoch 5: test: {'accuracy': 0.912}\n",
      "epoch 5: test: {'accuracy': 0.912}\n",
      " 70%|███████████████████████████▎           | 2191/3130 [05:51<02:15,  6.91it/s]06/28/2025 16:55:42 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.907}\n",
      "epoch 6: validation: {'accuracy': 0.907}\n",
      "06/28/2025 16:55:45 - INFO - __main__ - epoch 6: test: {'accuracy': 0.918}\n",
      "epoch 6: test: {'accuracy': 0.918}\n",
      " 80%|███████████████████████████████▏       | 2504/3130 [06:43<01:17,  8.04it/s]06/28/2025 16:56:34 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.907}\n",
      "epoch 7: validation: {'accuracy': 0.907}\n",
      "06/28/2025 16:56:37 - INFO - __main__ - epoch 7: test: {'accuracy': 0.918}\n",
      "epoch 7: test: {'accuracy': 0.918}\n",
      " 90%|███████████████████████████████████    | 2817/3130 [07:35<00:41,  7.53it/s]06/28/2025 16:57:26 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.906}\n",
      "epoch 8: validation: {'accuracy': 0.906}\n",
      "06/28/2025 16:57:29 - INFO - __main__ - epoch 8: test: {'accuracy': 0.918}\n",
      "epoch 8: test: {'accuracy': 0.918}\n",
      "100%|███████████████████████████████████████| 3130/3130 [08:27<00:00,  7.98it/s]06/28/2025 16:58:18 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.908}\n",
      "epoch 9: validation: {'accuracy': 0.908}\n",
      "06/28/2025 16:58:21 - INFO - __main__ - epoch 9: test: {'accuracy': 0.917}\n",
      "epoch 9: test: {'accuracy': 0.917}\n",
      "100%|███████████████████████████████████████| 3130/3130 [08:34<00:00,  6.08it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/agnews-6k/all-data \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k//val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86738bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
