{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd4481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: dill in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (0.32.4)\n",
      "Requirement already satisfied: packaging in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in d:\\research\\d2pruning\\env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\research\\d2pruning\\env\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\research\\d2pruning\\env\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\research\\d2pruning\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "# Install evaluate library for load_metric function\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ad982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in d:\\research\\d2pruning\\env\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\research\\d2pruning\\env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\research\\d2pruning\\env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf0b40",
   "metadata": {},
   "source": [
    "Run on ANLI-2k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/electra-small-discriminator\"\n",
    "\"FacebookAI/roberta-base\"\n",
    "\"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb280e3",
   "metadata": {},
   "source": [
    "Train on full to get training dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27205ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/07/2025 19:11:27 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 2000 to 1801 for creating validation set\n",
      "/storage/nammt/d2pruning/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"goddawg/anli-2k\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json\n",
      "loading file merges.txt from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors\n",
      "Some weights of the model checkpoint at FacebookAI/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 1801/1801 [00:00<00:00, 3308.46 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 3823.66 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 3518.28 examples/s\n",
      "Running tokenizer on dataset: 100%|██| 199/199 [00:00<00:00, 3640.30 examples/s]\n",
      "07/07/2025 19:11:39 - INFO - __main__ - Sample 1309 of the training set: (1309, {'input_ids': [0, 133, 27084, 26, 182, 2563, 14, 114, 47, 58, 7, 7581, 5, 394, 18, 563, 6, 89, 74, 28, 10, 68, 176, 4700, 4683, 11, 3574, 2010, 6, 142, 452, 18, 1138, 582, 11, 7, 5, 467, 13, 452, 18, 21156, 4, 178, 5, 27084, 26, 480, 14, 18, 5, 9588, 8587, 1387, 131, 24, 18, 10094, 480, 51, 26, 14, 89, 74, 33, 7, 28, 10, 847, 11, 1795, 9, 564, 135, 7, 843, 135, 4, 2, 2, 133, 3574, 2010, 467, 16, 6140, 30, 595, 1138, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "07/07/2025 19:11:39 - INFO - __main__ - Sample 228 of the training set: (228, {'input_ids': [0, 40420, 19508, 7573, 6, 22105, 3361, 7573, 36, 438, 4, 501, 4671, 126, 740, 4, 379, 4708, 73, 4429, 43, 21, 4645, 22595, 3361, 7, 4858, 35959, 18, 408, 35, 8334, 2708, 6, 8334, 4690, 6, 4858, 21872, 20512, 8, 2804, 7393, 4, 20, 737, 9, 4645, 22595, 3361, 11, 69, 183, 34304, 540, 14, 9, 5, 1406, 2297, 1114, 9, 10, 11738, 3361, 6, 55, 14, 9, 10, 295, 13749, 4, 2, 2, 40420, 19508, 7573, 6640, 69, 14349, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}).\n",
      "07/07/2025 19:11:39 - INFO - __main__ - Sample 51 of the training set: (51, {'input_ids': [0, 250, 9103, 10704, 11625, 41552, 3809, 15698, 23031, 21, 553, 7, 311, 5, 92, 1294, 198, 4, 91, 21, 7, 2067, 11, 5, 334, 558, 13, 10, 1294, 1440, 11625, 4, 34844, 13, 5, 1294, 7, 5240, 37, 12267, 99, 51, 74, 28, 101, 4, 2668, 9159, 5, 621, 74, 28, 6764, 101, 123, 8, 10, 2143, 4, 520, 5, 621, 1747, 2035, 24, 21, 765, 1816, 7001, 70, 11, 2440, 4, 2, 2, 20, 92, 1294, 21, 45, 38187, 260, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}).\n",
      "07/07/2025 19:11:41 - INFO - __main__ - ***** Running training *****\n",
      "07/07/2025 19:11:41 - INFO - __main__ -   Num examples = 1801\n",
      "07/07/2025 19:11:41 - INFO - __main__ -   Num Epochs = 10\n",
      "07/07/2025 19:11:41 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "07/07/2025 19:11:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "07/07/2025 19:11:41 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "07/07/2025 19:11:41 - INFO - __main__ -   Total optimization steps = 1130\n",
      "  0%|                                                  | 0/1130 [00:00<?, ?it/s]07/07/2025 19:11:41 - INFO - __main__ - Saving initial weights for reproducibility check...\n",
      "07/07/2025 19:11:42 - INFO - __main__ - Initial weights saved to initial_weights.pt\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "07/07/2025 19:11:43 - INFO - __main__ - Saving artifacts from the first training step...\n",
      "07/07/2025 19:11:43 - INFO - __main__ - First batch saved to first_batch.pt\n",
      "07/07/2025 19:11:45 - INFO - __main__ - First gradients saved to first_grads.pt\n",
      "  0%|                                        | 1/1130 [00:04<1:18:37,  4.18s/it]07/07/2025 19:11:46 - INFO - __main__ - First updated weights saved to first_updated_weights.pt\n",
      "07/07/2025 19:11:46 - INFO - __main__ - All debug checkpoints saved. Exiting now.\n",
      "  0%|                                        | 1/1130 [00:05<1:49:24,  5.81s/it]\n"
     ]
    }
   ],
   "source": [
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"FacebookAI/roberta-base\" \\\n",
    "    --output_dir ./data-model/anli-2k/all-data \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --save_debug_checkpoints \\\n",
    "    --debug_prefix \"run2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7112f678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing first_grads.pt ... MATCH\n",
      "Comparing first_updated_weights.pt ... MATCH\n",
      "Comparing initial_weights.pt ... MATCH\n"
     ]
    }
   ],
   "source": [
    "!python verify.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71f8f71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing first_batch.pt ... MATCH\n"
     ]
    }
   ],
   "source": [
    "!python verify_batch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf97f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71668ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/storage/nammt/d2pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/nammt/d2pruning/env/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/07/2025 10:25:40 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "README.md: 100%|███████████████████████████████| 818/818 [00:00<00:00, 3.31MB/s]\n",
      "data/train-00000-of-00001.parquet: 100%|██████| 584k/584k [00:01<00:00, 332kB/s]\n",
      "data/dev-00000-of-00001.parquet: 100%|████████| 369k/369k [00:01<00:00, 245kB/s]\n",
      "data/test-00000-of-00001.parquet: 100%|███████| 366k/366k [00:01<00:00, 248kB/s]\n",
      "Generating train split: 100%|██████| 2000/2000 [00:00<00:00, 9342.27 examples/s]\n",
      "Generating dev split: 100%|███████| 1000/1000 [00:00<00:00, 57325.08 examples/s]\n",
      "Generating test split: 100%|██████| 1000/1000 [00:00<00:00, 44786.54 examples/s]\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 2000 to 1801 for creating validation set\n",
      "/storage/nammt/d2pruning/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"goddawg/anli-2k\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json\n",
      "loading file merges.txt from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors\n",
      "Some weights of the model checkpoint at FacebookAI/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 1801/1801 [00:01<00:00, 1381.86 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 1746.93 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 1303.77 examples/s\n",
      "Running tokenizer on dataset: 100%|██| 199/199 [00:00<00:00, 1599.73 examples/s]\n",
      "07/07/2025 10:26:07 - INFO - __main__ - Sample 1309 of the training set: (1309, {'input_ids': [0, 133, 27084, 26, 182, 2563, 14, 114, 47, 58, 7, 7581, 5, 394, 18, 563, 6, 89, 74, 28, 10, 68, 176, 4700, 4683, 11, 3574, 2010, 6, 142, 452, 18, 1138, 582, 11, 7, 5, 467, 13, 452, 18, 21156, 4, 178, 5, 27084, 26, 480, 14, 18, 5, 9588, 8587, 1387, 131, 24, 18, 10094, 480, 51, 26, 14, 89, 74, 33, 7, 28, 10, 847, 11, 1795, 9, 564, 135, 7, 843, 135, 4, 2, 2, 133, 3574, 2010, 467, 16, 6140, 30, 595, 1138, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "07/07/2025 10:26:07 - INFO - __main__ - Sample 228 of the training set: (228, {'input_ids': [0, 40420, 19508, 7573, 6, 22105, 3361, 7573, 36, 438, 4, 501, 4671, 126, 740, 4, 379, 4708, 73, 4429, 43, 21, 4645, 22595, 3361, 7, 4858, 35959, 18, 408, 35, 8334, 2708, 6, 8334, 4690, 6, 4858, 21872, 20512, 8, 2804, 7393, 4, 20, 737, 9, 4645, 22595, 3361, 11, 69, 183, 34304, 540, 14, 9, 5, 1406, 2297, 1114, 9, 10, 11738, 3361, 6, 55, 14, 9, 10, 295, 13749, 4, 2, 2, 40420, 19508, 7573, 6640, 69, 14349, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}).\n",
      "07/07/2025 10:26:07 - INFO - __main__ - Sample 51 of the training set: (51, {'input_ids': [0, 250, 9103, 10704, 11625, 41552, 3809, 15698, 23031, 21, 553, 7, 311, 5, 92, 1294, 198, 4, 91, 21, 7, 2067, 11, 5, 334, 558, 13, 10, 1294, 1440, 11625, 4, 34844, 13, 5, 1294, 7, 5240, 37, 12267, 99, 51, 74, 28, 101, 4, 2668, 9159, 5, 621, 74, 28, 6764, 101, 123, 8, 10, 2143, 4, 520, 5, 621, 1747, 2035, 24, 21, 765, 1816, 7001, 70, 11, 2440, 4, 2, 2, 20, 92, 1294, 21, 45, 38187, 260, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}).\n",
      "07/07/2025 10:26:09 - INFO - __main__ - ***** Running training *****\n",
      "07/07/2025 10:26:09 - INFO - __main__ -   Num examples = 1801\n",
      "07/07/2025 10:26:09 - INFO - __main__ -   Num Epochs = 10\n",
      "07/07/2025 10:26:09 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "07/07/2025 10:26:09 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "07/07/2025 10:26:09 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "07/07/2025 10:26:09 - INFO - __main__ -   Total optimization steps = 1130\n",
      "  0%|                                                  | 0/1130 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████                                    | 113/1130 [00:30<03:48,  4.45it/s]07/07/2025 10:26:41 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.32160804020100503}\n",
      "epoch 0: validation: {'accuracy': 0.32160804020100503}\n",
      "07/07/2025 10:26:51 - INFO - __main__ - epoch 0: test: {'accuracy': 0.334}\n",
      "epoch 0: test: {'accuracy': 0.334}\n",
      " 20%|████████                                | 226/1130 [01:07<03:04,  4.90it/s]07/07/2025 10:27:19 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 1: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:27:28 - INFO - __main__ - epoch 1: test: {'accuracy': 0.327}\n",
      "epoch 1: test: {'accuracy': 0.327}\n",
      " 30%|████████████                            | 339/1130 [01:45<03:00,  4.37it/s]07/07/2025 10:27:57 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 2: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:28:06 - INFO - __main__ - epoch 2: test: {'accuracy': 0.327}\n",
      "epoch 2: test: {'accuracy': 0.327}\n",
      " 40%|████████████████                        | 452/1130 [02:22<02:30,  4.49it/s]07/07/2025 10:28:34 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 3: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:28:43 - INFO - __main__ - epoch 3: test: {'accuracy': 0.327}\n",
      "epoch 3: test: {'accuracy': 0.327}\n",
      " 50%|████████████████████                    | 565/1130 [03:00<02:10,  4.33it/s]07/07/2025 10:29:12 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 4: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:29:22 - INFO - __main__ - epoch 4: test: {'accuracy': 0.327}\n",
      "epoch 4: test: {'accuracy': 0.327}\n",
      " 60%|████████████████████████                | 678/1130 [03:38<01:31,  4.96it/s]07/07/2025 10:29:49 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 5: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:29:59 - INFO - __main__ - epoch 5: test: {'accuracy': 0.327}\n",
      "epoch 5: test: {'accuracy': 0.327}\n",
      " 70%|████████████████████████████            | 791/1130 [04:13<01:06,  5.11it/s]07/07/2025 10:30:24 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 6: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:30:28 - INFO - __main__ - epoch 6: test: {'accuracy': 0.327}\n",
      "epoch 6: test: {'accuracy': 0.327}\n",
      " 80%|████████████████████████████████        | 904/1130 [04:41<00:41,  5.42it/s]07/07/2025 10:30:52 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 7: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:30:56 - INFO - __main__ - epoch 7: test: {'accuracy': 0.327}\n",
      "epoch 7: test: {'accuracy': 0.327}\n",
      " 90%|███████████████████████████████████    | 1017/1130 [05:10<00:20,  5.55it/s]07/07/2025 10:31:20 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 8: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:31:25 - INFO - __main__ - epoch 8: test: {'accuracy': 0.327}\n",
      "epoch 8: test: {'accuracy': 0.327}\n",
      "100%|███████████████████████████████████████| 1130/1130 [05:37<00:00,  5.19it/s]07/07/2025 10:31:48 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 9: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:31:52 - INFO - __main__ - epoch 9: test: {'accuracy': 0.327}\n",
      "epoch 9: test: {'accuracy': 0.327}\n",
      "100%|███████████████████████████████████████| 1130/1130 [05:43<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"FacebookAI/roberta-base\" \\\n",
    "    --output_dir ./data-model/anli-2k/all-data \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38710cc",
   "metadata": {},
   "source": [
    "Save feature embeddings and importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aec4363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/storage/nammt/d2pruning\n",
      "07/07/2025 10:35:05 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 2000 to 1801 for creating validation set\n",
      "/storage/nammt/d2pruning/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/anli-2k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 1801/1801 [00:00<00:00, 3078.76 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 2163.33 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 2364.70 examples/s\n",
      "Running tokenizer on dataset: 100%|██| 199/199 [00:00<00:00, 2178.95 examples/s]\n",
      "07/07/2025 10:35:16 - INFO - __main__ - Sample 1309 of the training set: {'input_ids': [101, 1996, 17324, 2080, 2056, 2200, 4415, 2008, 2065, 2017, 2020, 2000, 11092, 1996, 2343, 1005, 1055, 2933, 1010, 2045, 2052, 2022, 1037, 1002, 1016, 23458, 4920, 1999, 2591, 3036, 1010, 2138, 2651, 1005, 1055, 3667, 3477, 1999, 2000, 1996, 2291, 2005, 2651, 1005, 1055, 11036, 2229, 1012, 1998, 1996, 17324, 2080, 2056, 1011, 1011, 2008, 1005, 1055, 1996, 7740, 5166, 2436, 1025, 2009, 1005, 1055, 12170, 26053, 1011, 1011, 2027, 2056, 2008, 2045, 2052, 2031, 2000, 2022, 1037, 3013, 1999, 6666, 1997, 2423, 3867, 2000, 2871, 3867, 1012, 102, 1996, 2591, 3036, 2291, 2003, 6787, 2011, 2783, 3667, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "07/07/2025 10:35:16 - INFO - __main__ - Sample 228 of the training set: {'input_ids': [101, 5545, 8527, 1010, 21479, 8527, 1006, 1039, 1012, 16333, 2620, 1516, 1039, 1012, 14168, 2487, 1013, 4720, 1007, 2001, 3203, 21208, 7971, 2000, 2888, 9937, 1005, 1055, 2336, 1024, 4615, 2984, 1010, 4615, 3870, 1010, 2888, 21870, 1998, 3159, 3487, 1012, 1996, 2597, 1997, 3203, 21208, 7971, 1999, 2014, 2154, 15881, 2625, 2008, 1997, 1996, 2759, 2715, 2801, 1997, 1037, 21208, 7971, 1010, 2062, 2008, 1997, 1037, 19174, 1012, 102, 5545, 8527, 4669, 2014, 6139, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
      "07/07/2025 10:35:16 - INFO - __main__ - Sample 51 of the training set: {'input_ids': [101, 1037, 2611, 2171, 11531, 1026, 7987, 1028, 5199, 2001, 2356, 2000, 2265, 1996, 2047, 3076, 2105, 1012, 2002, 2001, 2000, 3524, 1999, 1996, 2082, 2436, 2005, 1037, 3076, 2315, 11531, 1012, 3403, 2005, 1996, 3076, 2000, 7180, 2002, 4999, 2054, 2027, 2052, 2022, 2066, 1012, 5199, 5071, 1996, 2711, 2052, 2022, 4206, 2066, 2032, 1998, 1037, 2879, 1012, 2043, 1996, 2711, 2633, 3369, 2009, 2001, 2460, 2611, 5102, 2035, 1999, 2630, 1012, 102, 1996, 2047, 3076, 2001, 2025, 2137, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.\n",
      "07/07/2025 10:35:18 - INFO - __main__ - ***** Running evaluation *****\n",
      "07/07/2025 10:35:18 - INFO - __main__ -   Num examples = 1801\n",
      "07/07/2025 10:35:18 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "07/07/2025 10:35:18 - INFO - __main__ -   Total train batch size (w. parallel & distributed) = 16\n",
      "0it [00:00, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "113it [00:10, 10.29it/s]\n",
      "07/07/2025 10:35:29 - INFO - __main__ - result: {'accuracy': 0.3192670738478623}\n",
      "07/07/2025 10:35:29 - INFO - __main__ - Saved confidence values for 1801 samples at ./data-model/anli-2k/all-data/train_confs.npy\n",
      "07/07/2025 10:35:29 - INFO - __main__ - Saved feature embeddings for 1801 samples at ./data-model/anli-2k/all-data/train-features.npy\n",
      "0\n",
      "0\n",
      "Calculating LOF importance scores...\n",
      "LOF importance scores calculated and stored.\n",
      "Saving data score at ./data-model/anli-2k/all-data/data-score-goddawg.pickle\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python3 train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/anli-2k/all-data \\\n",
    "    --do_eval \\\n",
    "    --eval_train \\\n",
    "    --save_feature \\\n",
    "    --save_confidence \\\n",
    "    --save_importance_scores \\\n",
    "    --training_dynamics \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288b5c3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "Prun and train on 50% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "459148a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/storage/nammt/d2pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/nammt/d2pruning/env/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/07/2025 11:31:12 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 2000 to 1801 for creating validation set\n",
      "Using descending order\n",
      "***************** Getting coresets for label 0  ********************\n",
      "Random selection.\n",
      "***************** Getting coresets for label 1  ********************\n",
      "Random selection.\n",
      "***************** Getting coresets for label 2  ********************\n",
      "Random selection.\n",
      "07/07/2025 11:31:19 - INFO - __main__ - Pruned dataset from 1801 samples to 900 samples, retaining 0.5 of the dataset.\n",
      "['uid', 'premise', 'hypothesis', 'label', 'reason'] ['uid', 'premise', 'hypothesis', 'label', 'reason']\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
      "        num_rows: 199\n",
      "    })\n",
      "})\n",
      "Pruned 1801 samples in original train set to 900\n",
      "/storage/nammt/d2pruning/train_nlp_explore.py:913: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  np.save(coreset_index_path, np.array(coreset_index))\n",
      "/storage/nammt/d2pruning/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/anli-2k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|██| 900/900 [00:00<00:00, 2584.79 examples/s]\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 2585.89 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 2732.26 examples/s\n",
      "Running tokenizer on dataset: 100%|██| 199/199 [00:00<00:00, 2798.17 examples/s]\n",
      "07/07/2025 11:31:23 - INFO - __main__ - Sample 654 of the training set: {'input_ids': [101, 2720, 2585, 11458, 1010, 2132, 1997, 13938, 1005, 1055, 2334, 3136, 5496, 2720, 21037, 2102, 8040, 8093, 29099, 2121, 1010, 3539, 2704, 1997, 2896, 13019, 1998, 1037, 2266, 1997, 1996, 1058, 2860, 26653, 2604, 1010, 1997, 2667, 2000, 2224, 2010, 2576, 3635, 2000, 3747, 1996, 9751, 2011, 2110, 19608, 1999, 16399, 2046, 4447, 1997, 3919, 21003, 2114, 13938, 1012, 102, 2880, 9751, 2046, 4447, 1997, 3919, 21003, 2114, 13938, 2020, 4146, 2011, 21037, 2102, 8040, 8093, 29099, 2121, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.\n",
      "07/07/2025 11:31:23 - INFO - __main__ - Sample 114 of the training set: {'input_ids': [101, 3515, 21472, 23686, 1010, 10864, 8259, 3055, 1040, 2100, 1010, 2003, 1037, 6351, 25560, 12175, 2013, 1996, 6058, 2555, 1997, 1996, 12175, 5583, 1010, 3155, 2570, 7338, 1999, 6705, 1012, 2009, 2001, 3603, 2006, 2484, 2337, 3055, 1010, 2011, 5569, 15211, 9865, 2378, 2720, 15710, 2012, 1996, 2148, 18063, 1047, 7485, 9970, 1999, 1996, 5569, 3072, 1012, 1996, 12175, 2001, 2315, 2005, 2137, 3364, 6266, 23686, 1012, 102, 9865, 2378, 2720, 15710, 2003, 2006, 5569, 10662, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "07/07/2025 11:31:23 - INFO - __main__ - Sample 25 of the training set: {'input_ids': [101, 1996, 15041, 1026, 7987, 1028, 1996, 4268, 2734, 2000, 6140, 2070, 4981, 2005, 2082, 1012, 1996, 15041, 2921, 15451, 11263, 27989, 2075, 1012, 2027, 2699, 2673, 2027, 2071, 2079, 1012, 2027, 2435, 2039, 1998, 2939, 2000, 1996, 3075, 1012, 2027, 6267, 1996, 5491, 2045, 1012, 102, 2044, 1996, 2336, 2071, 2025, 6140, 2082, 4981, 2027, 2109, 1996, 2028, 2012, 1996, 3075, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "07/07/2025 11:31:25 - INFO - __main__ - ***** Running training *****\n",
      "07/07/2025 11:31:25 - INFO - __main__ -   Num examples = 900\n",
      "07/07/2025 11:31:25 - INFO - __main__ -   Num Epochs = 10\n",
      "07/07/2025 11:31:25 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
      "07/07/2025 11:31:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "07/07/2025 11:31:25 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "07/07/2025 11:31:25 - INFO - __main__ -   Total optimization steps = 290\n",
      "  0%|                                                   | 0/290 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████▏                                     | 29/290 [00:13<01:54,  2.28it/s]07/07/2025 11:31:40 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.36683417085427134}\n",
      "epoch 0: validation: {'accuracy': 0.36683417085427134}\n",
      "07/07/2025 11:31:45 - INFO - __main__ - epoch 0: test: {'accuracy': 0.327}\n",
      "epoch 0: test: {'accuracy': 0.327}\n",
      " 20%|████████▍                                 | 58/290 [00:33<01:51,  2.09it/s]07/07/2025 11:32:00 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.4321608040201005}\n",
      "epoch 1: validation: {'accuracy': 0.4321608040201005}\n",
      "07/07/2025 11:32:05 - INFO - __main__ - epoch 1: test: {'accuracy': 0.332}\n",
      "epoch 1: test: {'accuracy': 0.332}\n",
      " 30%|████████████▌                             | 87/290 [00:52<01:48,  1.88it/s]07/07/2025 11:32:19 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.4824120603015075}\n",
      "epoch 2: validation: {'accuracy': 0.4824120603015075}\n",
      "07/07/2025 11:32:25 - INFO - __main__ - epoch 2: test: {'accuracy': 0.345}\n",
      "epoch 2: test: {'accuracy': 0.345}\n",
      " 40%|████████████████▍                        | 116/290 [01:12<01:21,  2.12it/s]07/07/2025 11:32:39 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.5025125628140703}\n",
      "epoch 3: validation: {'accuracy': 0.5025125628140703}\n",
      "07/07/2025 11:32:44 - INFO - __main__ - epoch 3: test: {'accuracy': 0.347}\n",
      "epoch 3: test: {'accuracy': 0.347}\n",
      " 50%|████████████████████▌                    | 145/290 [01:32<01:09,  2.08it/s]07/07/2025 11:32:59 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.5025125628140703}\n",
      "epoch 4: validation: {'accuracy': 0.5025125628140703}\n",
      "07/07/2025 11:33:04 - INFO - __main__ - epoch 4: test: {'accuracy': 0.337}\n",
      "epoch 4: test: {'accuracy': 0.337}\n",
      " 60%|████████████████████████▌                | 174/290 [01:52<00:56,  2.05it/s]07/07/2025 11:33:19 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.48743718592964824}\n",
      "epoch 5: validation: {'accuracy': 0.48743718592964824}\n",
      "07/07/2025 11:33:24 - INFO - __main__ - epoch 5: test: {'accuracy': 0.351}\n",
      "epoch 5: test: {'accuracy': 0.351}\n",
      " 70%|████████████████████████████▋            | 203/290 [02:12<00:38,  2.26it/s]07/07/2025 11:33:39 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.49246231155778897}\n",
      "epoch 6: validation: {'accuracy': 0.49246231155778897}\n",
      "07/07/2025 11:33:45 - INFO - __main__ - epoch 6: test: {'accuracy': 0.353}\n",
      "epoch 6: test: {'accuracy': 0.353}\n",
      " 80%|████████████████████████████████▊        | 232/290 [02:32<00:26,  2.20it/s]07/07/2025 11:33:59 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.4723618090452261}\n",
      "epoch 7: validation: {'accuracy': 0.4723618090452261}\n",
      "07/07/2025 11:34:04 - INFO - __main__ - epoch 7: test: {'accuracy': 0.344}\n",
      "epoch 7: test: {'accuracy': 0.344}\n",
      " 90%|████████████████████████████████████▉    | 261/290 [02:52<00:13,  2.15it/s]07/07/2025 11:34:19 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.48743718592964824}\n",
      "epoch 8: validation: {'accuracy': 0.48743718592964824}\n",
      "07/07/2025 11:34:25 - INFO - __main__ - epoch 8: test: {'accuracy': 0.352}\n",
      "epoch 8: test: {'accuracy': 0.352}\n",
      "100%|█████████████████████████████████████████| 290/290 [03:13<00:00,  2.26it/s]07/07/2025 11:34:39 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.4824120603015075}\n",
      "epoch 9: validation: {'accuracy': 0.4824120603015075}\n",
      "07/07/2025 11:34:45 - INFO - __main__ - epoch 9: test: {'accuracy': 0.354}\n",
      "epoch 9: test: {'accuracy': 0.354}\n",
      "100%|█████████████████████████████████████████| 290/290 [03:19<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "CORESET_RATIO = 0.5\n",
    "N_NEIGHBOR = 10\n",
    "GAMMA = 1\n",
    "\n",
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/anli-2k/coreset-0.5 \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --coreset \\\n",
    "    --coreset-mode random \\\n",
    "    --budget-mode uniform \\\n",
    "    --sampling-mode graph \\\n",
    "    --data-score-path ./data-model/anli-2k/all-data/data-score-goddawg.pickle \\\n",
    "    --feature-path ./data-model/anli-2k/all-data/train-features.npy \\\n",
    "    --coreset-key forgetting \\\n",
    "    --coreset-ratio {CORESET_RATIO} \\\n",
    "    --mis-ratio 0.4 \\\n",
    "    --label-balanced \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --n-neighbor {N_NEIGHBOR} \\\n",
    "    --gamma {GAMMA} \\\n",
    "    --graph-mode sum \\\n",
    "    --graph-sampling-mode weighted \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888302e0",
   "metadata": {},
   "source": [
    "Train D(Full data) and D'(50% coreset) on BERT base uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ff245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/storage/nammt/d2pruning\n",
      "07/07/2025 10:57:44 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 2000 to 1801 for creating validation set\n",
      "/storage/nammt/d2pruning/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"goddawg/anli-2k\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json\n",
      "loading file merges.txt from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors\n",
      "Some weights of the model checkpoint at FacebookAI/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 1801/1801 [00:00<00:00, 2740.94 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 3168.66 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 3280.30 examples/s\n",
      "Running tokenizer on dataset: 100%|██| 199/199 [00:00<00:00, 2875.56 examples/s]\n",
      "07/07/2025 10:57:56 - INFO - __main__ - Sample 1309 of the training set: (1309, {'input_ids': [0, 133, 27084, 26, 182, 2563, 14, 114, 47, 58, 7, 7581, 5, 394, 18, 563, 6, 89, 74, 28, 10, 68, 176, 4700, 4683, 11, 3574, 2010, 6, 142, 452, 18, 1138, 582, 11, 7, 5, 467, 13, 452, 18, 21156, 4, 178, 5, 27084, 26, 480, 14, 18, 5, 9588, 8587, 1387, 131, 24, 18, 10094, 480, 51, 26, 14, 89, 74, 33, 7, 28, 10, 847, 11, 1795, 9, 564, 135, 7, 843, 135, 4, 2, 2, 133, 3574, 2010, 467, 16, 6140, 30, 595, 1138, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "07/07/2025 10:57:56 - INFO - __main__ - Sample 228 of the training set: (228, {'input_ids': [0, 40420, 19508, 7573, 6, 22105, 3361, 7573, 36, 438, 4, 501, 4671, 126, 740, 4, 379, 4708, 73, 4429, 43, 21, 4645, 22595, 3361, 7, 4858, 35959, 18, 408, 35, 8334, 2708, 6, 8334, 4690, 6, 4858, 21872, 20512, 8, 2804, 7393, 4, 20, 737, 9, 4645, 22595, 3361, 11, 69, 183, 34304, 540, 14, 9, 5, 1406, 2297, 1114, 9, 10, 11738, 3361, 6, 55, 14, 9, 10, 295, 13749, 4, 2, 2, 40420, 19508, 7573, 6640, 69, 14349, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}).\n",
      "07/07/2025 10:57:56 - INFO - __main__ - Sample 51 of the training set: (51, {'input_ids': [0, 250, 9103, 10704, 11625, 41552, 3809, 15698, 23031, 21, 553, 7, 311, 5, 92, 1294, 198, 4, 91, 21, 7, 2067, 11, 5, 334, 558, 13, 10, 1294, 1440, 11625, 4, 34844, 13, 5, 1294, 7, 5240, 37, 12267, 99, 51, 74, 28, 101, 4, 2668, 9159, 5, 621, 74, 28, 6764, 101, 123, 8, 10, 2143, 4, 520, 5, 621, 1747, 2035, 24, 21, 765, 1816, 7001, 70, 11, 2440, 4, 2, 2, 20, 92, 1294, 21, 45, 38187, 260, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}).\n",
      "07/07/2025 10:57:57 - INFO - __main__ - ***** Running training *****\n",
      "07/07/2025 10:57:57 - INFO - __main__ -   Num examples = 1801\n",
      "07/07/2025 10:57:57 - INFO - __main__ -   Num Epochs = 10\n",
      "07/07/2025 10:57:57 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "07/07/2025 10:57:57 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "07/07/2025 10:57:57 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "07/07/2025 10:57:57 - INFO - __main__ -   Total optimization steps = 1130\n",
      "  0%|                                                  | 0/1130 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████                                    | 113/1130 [00:27<03:21,  5.05it/s]07/07/2025 10:58:26 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.32160804020100503}\n",
      "epoch 0: validation: {'accuracy': 0.32160804020100503}\n",
      "07/07/2025 10:58:31 - INFO - __main__ - epoch 0: test: {'accuracy': 0.334}\n",
      "epoch 0: test: {'accuracy': 0.334}\n",
      " 20%|████████                                | 226/1130 [01:01<03:46,  3.98it/s]07/07/2025 10:59:00 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 1: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:59:06 - INFO - __main__ - epoch 1: test: {'accuracy': 0.327}\n",
      "epoch 1: test: {'accuracy': 0.327}\n",
      " 30%|████████████                            | 339/1130 [01:36<03:09,  4.17it/s]07/07/2025 10:59:35 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 2: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 10:59:40 - INFO - __main__ - epoch 2: test: {'accuracy': 0.327}\n",
      "epoch 2: test: {'accuracy': 0.327}\n",
      " 40%|████████████████                        | 452/1130 [02:11<02:38,  4.29it/s]07/07/2025 11:00:10 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 3: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 11:00:15 - INFO - __main__ - epoch 3: test: {'accuracy': 0.327}\n",
      "epoch 3: test: {'accuracy': 0.327}\n",
      " 50%|████████████████████                    | 565/1130 [02:46<02:14,  4.21it/s]07/07/2025 11:00:45 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 4: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 11:00:50 - INFO - __main__ - epoch 4: test: {'accuracy': 0.327}\n",
      "epoch 4: test: {'accuracy': 0.327}\n",
      " 60%|████████████████████████                | 678/1130 [03:20<01:46,  4.25it/s]07/07/2025 11:01:19 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 5: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 11:01:25 - INFO - __main__ - epoch 5: test: {'accuracy': 0.327}\n",
      "epoch 5: test: {'accuracy': 0.327}\n",
      " 70%|████████████████████████████            | 791/1130 [03:55<01:21,  4.17it/s]07/07/2025 11:01:54 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 6: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 11:01:59 - INFO - __main__ - epoch 6: test: {'accuracy': 0.327}\n",
      "epoch 6: test: {'accuracy': 0.327}\n",
      " 80%|████████████████████████████████        | 904/1130 [04:30<00:52,  4.32it/s]07/07/2025 11:02:29 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 7: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 11:02:34 - INFO - __main__ - epoch 7: test: {'accuracy': 0.327}\n",
      "epoch 7: test: {'accuracy': 0.327}\n",
      " 90%|███████████████████████████████████    | 1017/1130 [05:04<00:25,  4.49it/s]07/07/2025 11:03:03 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 8: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 11:03:09 - INFO - __main__ - epoch 8: test: {'accuracy': 0.327}\n",
      "epoch 8: test: {'accuracy': 0.327}\n",
      "100%|███████████████████████████████████████| 1130/1130 [05:39<00:00,  4.37it/s]07/07/2025 11:03:38 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.39195979899497485}\n",
      "epoch 9: validation: {'accuracy': 0.39195979899497485}\n",
      "07/07/2025 11:03:44 - INFO - __main__ - epoch 9: test: {'accuracy': 0.327}\n",
      "epoch 9: test: {'accuracy': 0.327}\n",
      "100%|███████████████████████████████████████| 1130/1130 [05:46<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"FacebookAI/roberta-base\" \\\n",
    "    --output_dir ./data-model/anli-2k/roberta_base/bert/ \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda05b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/storage/nammt/d2pruning\n",
      "07/07/2025 11:35:46 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 2000 to 1801 for creating validation set\n",
      "/storage/nammt/d2pruning/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/anli-2k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|██| 900/900 [00:00<00:00, 1804.46 examples/s]\n",
      "Running tokenizer on dataset: 100%|██| 199/199 [00:00<00:00, 1412.37 examples/s]\n",
      "07/07/2025 11:35:59 - INFO - __main__ - Sample 654 of the training set: (654, {'input_ids': [101, 2720, 2585, 11458, 1010, 2132, 1997, 13938, 1005, 1055, 2334, 3136, 5496, 2720, 21037, 2102, 8040, 8093, 29099, 2121, 1010, 3539, 2704, 1997, 2896, 13019, 1998, 1037, 2266, 1997, 1996, 1058, 2860, 26653, 2604, 1010, 1997, 2667, 2000, 2224, 2010, 2576, 3635, 2000, 3747, 1996, 9751, 2011, 2110, 19608, 1999, 16399, 2046, 4447, 1997, 3919, 21003, 2114, 13938, 1012, 102, 2880, 9751, 2046, 4447, 1997, 3919, 21003, 2114, 13938, 2020, 4146, 2011, 21037, 2102, 8040, 8093, 29099, 2121, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}).\n",
      "07/07/2025 11:35:59 - INFO - __main__ - Sample 114 of the training set: (114, {'input_ids': [101, 3515, 21472, 23686, 1010, 10864, 8259, 3055, 1040, 2100, 1010, 2003, 1037, 6351, 25560, 12175, 2013, 1996, 6058, 2555, 1997, 1996, 12175, 5583, 1010, 3155, 2570, 7338, 1999, 6705, 1012, 2009, 2001, 3603, 2006, 2484, 2337, 3055, 1010, 2011, 5569, 15211, 9865, 2378, 2720, 15710, 2012, 1996, 2148, 18063, 1047, 7485, 9970, 1999, 1996, 5569, 3072, 1012, 1996, 12175, 2001, 2315, 2005, 2137, 3364, 6266, 23686, 1012, 102, 9865, 2378, 2720, 15710, 2003, 2006, 5569, 10662, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "07/07/2025 11:35:59 - INFO - __main__ - Sample 25 of the training set: (25, {'input_ids': [101, 1996, 15041, 1026, 7987, 1028, 1996, 4268, 2734, 2000, 6140, 2070, 4981, 2005, 2082, 1012, 1996, 15041, 2921, 15451, 11263, 27989, 2075, 1012, 2027, 2699, 2673, 2027, 2071, 2079, 1012, 2027, 2435, 2039, 1998, 2939, 2000, 1996, 3075, 1012, 2027, 6267, 1996, 5491, 2045, 1012, 102, 2044, 1996, 2336, 2071, 2025, 6140, 2082, 4981, 2027, 2109, 1996, 2028, 2012, 1996, 3075, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "07/07/2025 11:36:01 - INFO - __main__ - ***** Running training *****\n",
      "07/07/2025 11:36:01 - INFO - __main__ -   Num examples = 900\n",
      "07/07/2025 11:36:01 - INFO - __main__ -   Num Epochs = 10\n",
      "07/07/2025 11:36:01 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "07/07/2025 11:36:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "07/07/2025 11:36:01 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "07/07/2025 11:36:01 - INFO - __main__ -   Total optimization steps = 570\n",
      "  0%|                                                   | 0/570 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████▏                                     | 57/570 [00:13<01:51,  4.58it/s]07/07/2025 11:36:16 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.3869346733668342}\n",
      "epoch 0: validation: {'accuracy': 0.3869346733668342}\n",
      "07/07/2025 11:36:21 - INFO - __main__ - epoch 0: test: {'accuracy': 0.319}\n",
      "epoch 0: test: {'accuracy': 0.319}\n",
      " 20%|████████▏                                | 114/570 [00:33<01:44,  4.35it/s]07/07/2025 11:36:36 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.37185929648241206}\n",
      "epoch 1: validation: {'accuracy': 0.37185929648241206}\n",
      "07/07/2025 11:36:41 - INFO - __main__ - epoch 1: test: {'accuracy': 0.324}\n",
      "epoch 1: test: {'accuracy': 0.324}\n",
      " 30%|████████████▎                            | 171/570 [00:54<01:43,  3.84it/s]07/07/2025 11:36:56 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.4371859296482412}\n",
      "epoch 2: validation: {'accuracy': 0.4371859296482412}\n",
      "07/07/2025 11:37:02 - INFO - __main__ - epoch 2: test: {'accuracy': 0.335}\n",
      "epoch 2: test: {'accuracy': 0.335}\n",
      " 40%|████████████████▍                        | 228/570 [01:15<01:16,  4.45it/s]07/07/2025 11:37:17 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.49748743718592964}\n",
      "epoch 3: validation: {'accuracy': 0.49748743718592964}\n",
      "07/07/2025 11:37:23 - INFO - __main__ - epoch 3: test: {'accuracy': 0.354}\n",
      "epoch 3: test: {'accuracy': 0.354}\n",
      " 50%|████████████████████▌                    | 285/570 [01:36<01:09,  4.10it/s]07/07/2025 11:37:38 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.47738693467336685}\n",
      "epoch 4: validation: {'accuracy': 0.47738693467336685}\n",
      "07/07/2025 11:37:44 - INFO - __main__ - epoch 4: test: {'accuracy': 0.351}\n",
      "epoch 4: test: {'accuracy': 0.351}\n",
      " 60%|████████████████████████▌                | 342/570 [01:57<00:52,  4.35it/s]07/07/2025 11:37:59 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.5728643216080402}\n",
      "epoch 5: validation: {'accuracy': 0.5728643216080402}\n",
      "07/07/2025 11:38:05 - INFO - __main__ - epoch 5: test: {'accuracy': 0.353}\n",
      "epoch 5: test: {'accuracy': 0.353}\n",
      " 70%|████████████████████████████▋            | 399/570 [02:17<00:37,  4.57it/s]07/07/2025 11:38:20 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.5376884422110553}\n",
      "epoch 6: validation: {'accuracy': 0.5376884422110553}\n",
      "07/07/2025 11:38:25 - INFO - __main__ - epoch 6: test: {'accuracy': 0.358}\n",
      "epoch 6: test: {'accuracy': 0.358}\n",
      " 80%|████████████████████████████████▊        | 456/570 [02:38<00:25,  4.39it/s]07/07/2025 11:38:40 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.5628140703517588}\n",
      "epoch 7: validation: {'accuracy': 0.5628140703517588}\n",
      "07/07/2025 11:38:46 - INFO - __main__ - epoch 7: test: {'accuracy': 0.362}\n",
      "epoch 7: test: {'accuracy': 0.362}\n",
      " 90%|████████████████████████████████████▉    | 513/570 [02:58<00:13,  4.30it/s]07/07/2025 11:39:01 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.5628140703517588}\n",
      "epoch 8: validation: {'accuracy': 0.5628140703517588}\n",
      "07/07/2025 11:39:06 - INFO - __main__ - epoch 8: test: {'accuracy': 0.365}\n",
      "epoch 8: test: {'accuracy': 0.365}\n",
      "100%|█████████████████████████████████████████| 570/570 [03:19<00:00,  4.55it/s]07/07/2025 11:39:21 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.5678391959798995}\n",
      "epoch 9: validation: {'accuracy': 0.5678391959798995}\n",
      "07/07/2025 11:39:26 - INFO - __main__ - epoch 9: test: {'accuracy': 0.363}\n",
      "epoch 9: test: {'accuracy': 0.363}\n",
      "100%|█████████████████████████████████████████| 570/570 [03:25<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/anli-2k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/anli-2k/bert_base/bert-50%/ \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/anli-2k/val_index.npy \\\n",
    "    --train-index-path ./data-model/anli-2k/coreset-0.5/coreset-goddawg.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb34aec",
   "metadata": {},
   "source": [
    "### Run on AGNews-5K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31713022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google-bert/bert-base-uncased'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"google/electra-small-discriminator\"\n",
    "\"FacebookAI/roberta-base\"\n",
    "\"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44be368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n",
      "06/28/2025 18:41:15 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 5000/5000 [00:00<00:00, 7799.81 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 7597.29 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8008.06 examples/s\n",
      "06/28/2025 18:41:24 - INFO - __main__ - Sample 912 of the training set: (912, {'input_ids': [101, 3956, 1005, 1055, 10666, 3210, 2039, 2490, 2005, 5622, 5283, 2094, 3789, 102, 6744, 1006, 26665, 1007, 1011, 5611, 3539, 2704, 16126, 10666, 8610, 2098, 2490, 2005, 9432, 1005, 1055, 5622, 5283, 2094, 2283, 3789, 2000, 3288, 1996, 4450, 2283, 2046, 1037, 2231, 2008, 2052, 3796, 10245, 3864, 1998, 6643, 3726, 1996, 2126, 2005, 1037, 14474, 10534, 2279, 2095, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "06/28/2025 18:41:24 - INFO - __main__ - Sample 204 of the training set: (204, {'input_ids': [101, 13420, 6971, 2039, 2007, 25353, 14905, 2937, 102, 2044, 3773, 2210, 3112, 2007, 1996, 4431, 5617, 2009, 2580, 2005, 7513, 1001, 4464, 1025, 1055, 26381, 4132, 1010, 13420, 2097, 2085, 2147, 2007, 25353, 14905, 2937, 2000, 3443, 4431, 7248, 2005, 1017, 2290, 2398, 8454, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "06/28/2025 18:41:24 - INFO - __main__ - Sample 2253 of the training set: (2253, {'input_ids': [101, 4654, 3540, 22879, 5668, 7523, 2322, 12954, 28397, 1999, 5279, 1006, 9706, 1007, 102, 9706, 1011, 4654, 3540, 22879, 5668, 3603, 2322, 23880, 12954, 28397, 1999, 1996, 13253, 3089, 3148, 18128, 1999, 2530, 5279, 1010, 1996, 2231, 1005, 1055, 2473, 1997, 21387, 2056, 9857, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "06/28/2025 18:41:26 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Num examples = 5000\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 18:41:26 - INFO - __main__ -   Total optimization steps = 3130\n",
      "  0%|                                                  | 0/3130 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|███▉                                    | 312/3130 [00:42<06:30,  7.22it/s]06/28/2025 18:42:12 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.896}\n",
      "epoch 0: validation: {'accuracy': 0.896}\n",
      "06/28/2025 18:42:15 - INFO - __main__ - epoch 0: test: {'accuracy': 0.906}\n",
      "epoch 0: test: {'accuracy': 0.906}\n",
      " 20%|███████▉                                | 625/3130 [01:33<05:22,  7.77it/s]06/28/2025 18:43:02 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.895}\n",
      "epoch 1: validation: {'accuracy': 0.895}\n",
      " 30%|████████████                            | 939/3130 [02:21<04:25,  8.24it/s]06/28/2025 18:43:50 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.908}\n",
      "epoch 2: validation: {'accuracy': 0.908}\n",
      "06/28/2025 18:43:53 - INFO - __main__ - epoch 2: test: {'accuracy': 0.913}\n",
      "epoch 2: test: {'accuracy': 0.913}\n",
      " 40%|███████████████▌                       | 1252/3130 [03:12<04:33,  6.87it/s]06/28/2025 18:44:41 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.904}\n",
      "epoch 3: validation: {'accuracy': 0.904}\n",
      " 50%|███████████████████▌                   | 1565/3130 [04:00<03:32,  7.38it/s]06/28/2025 18:45:29 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.905}\n",
      "epoch 4: validation: {'accuracy': 0.905}\n",
      " 60%|███████████████████████▍               | 1878/3130 [04:48<03:33,  5.86it/s]06/28/2025 18:46:18 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.914}\n",
      "epoch 5: validation: {'accuracy': 0.914}\n",
      "06/28/2025 18:46:21 - INFO - __main__ - epoch 5: test: {'accuracy': 0.911}\n",
      "epoch 5: test: {'accuracy': 0.911}\n",
      " 70%|███████████████████████████▎           | 2190/3130 [05:39<01:55,  8.17it/s]06/28/2025 18:47:08 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.903}\n",
      "epoch 6: validation: {'accuracy': 0.903}\n",
      " 80%|███████████████████████████████▏       | 2504/3130 [06:27<01:27,  7.14it/s]06/28/2025 18:47:56 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.907}\n",
      "epoch 7: validation: {'accuracy': 0.907}\n",
      " 90%|███████████████████████████████████    | 2817/3130 [07:14<00:36,  8.61it/s]06/28/2025 18:48:44 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.905}\n",
      "epoch 8: validation: {'accuracy': 0.905}\n",
      "100%|███████████████████████████████████████| 3130/3130 [08:02<00:00,  7.97it/s]06/28/2025 18:49:32 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.907}\n",
      "epoch 9: validation: {'accuracy': 0.907}\n",
      "100%|███████████████████████████████████████| 3130/3130 [08:06<00:00,  6.43it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/agnews-6k/all-data \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k//val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n",
      "06/28/2025 18:39:30 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 5000/5000 [00:00<00:00, 7755.22 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8042.38 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8217.34 examples/s\n",
      "06/28/2025 18:39:38 - INFO - __main__ - Sample 912 of the training set: {'input_ids': [101, 3956, 1005, 1055, 10666, 3210, 2039, 2490, 2005, 5622, 5283, 2094, 3789, 102, 6744, 1006, 26665, 1007, 1011, 5611, 3539, 2704, 16126, 10666, 8610, 2098, 2490, 2005, 9432, 1005, 1055, 5622, 5283, 2094, 2283, 3789, 2000, 3288, 1996, 4450, 2283, 2046, 1037, 2231, 2008, 2052, 3796, 10245, 3864, 1998, 6643, 3726, 1996, 2126, 2005, 1037, 14474, 10534, 2279, 2095, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "06/28/2025 18:39:38 - INFO - __main__ - Sample 204 of the training set: {'input_ids': [101, 13420, 6971, 2039, 2007, 25353, 14905, 2937, 102, 2044, 3773, 2210, 3112, 2007, 1996, 4431, 5617, 2009, 2580, 2005, 7513, 1001, 4464, 1025, 1055, 26381, 4132, 1010, 13420, 2097, 2085, 2147, 2007, 25353, 14905, 2937, 2000, 3443, 4431, 7248, 2005, 1017, 2290, 2398, 8454, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}.\n",
      "06/28/2025 18:39:38 - INFO - __main__ - Sample 2253 of the training set: {'input_ids': [101, 4654, 3540, 22879, 5668, 7523, 2322, 12954, 28397, 1999, 5279, 1006, 9706, 1007, 102, 9706, 1011, 4654, 3540, 22879, 5668, 3603, 2322, 23880, 12954, 28397, 1999, 1996, 13253, 3089, 3148, 18128, 1999, 2530, 5279, 1010, 1996, 2231, 1005, 1055, 2473, 1997, 21387, 2056, 9857, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}.\n",
      "06/28/2025 18:39:40 - INFO - __main__ - ***** Running evaluation *****\n",
      "06/28/2025 18:39:40 - INFO - __main__ -   Num examples = 5000\n",
      "06/28/2025 18:39:40 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 18:39:40 - INFO - __main__ -   Total train batch size (w. parallel & distributed) = 16\n",
      "0it [00:00, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "313it [00:10, 29.80it/s]\n",
      "06/28/2025 18:39:50 - INFO - __main__ - result: {'accuracy': 0.2532}\n",
      "06/28/2025 18:39:50 - INFO - __main__ - Saved confidence values for 5000 samples at ./data-model/agnews-6k//all-data/train_confs.npy\n",
      "06/28/2025 18:39:50 - INFO - __main__ - Saved feature embeddings for 5000 samples at ./data-model/agnews-6k//all-data/train-features.npy\n",
      "0\n",
      "0\n",
      "Calculating LOF importance scores...\n",
      "LOF importance scores calculated and stored.\n",
      "Saving data score at ./data-model/agnews-6k//all-data/data-score-goddawg.pickle\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"google/electra-small-discriminator\"\\\n",
    "    --output_dir ./data-model/agnews-6k//all-data \\\n",
    "    --do_eval \\\n",
    "    --eval_train \\\n",
    "    --save_feature \\\n",
    "    --save_confidence \\\n",
    "    --save_importance_scores \\\n",
    "    --training_dynamics \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k/val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aaddc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/28/2025 17:40:17 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "Using descending order\n",
      "Bad data -> High priority accumulated_margin: tensor([-9.8320, -8.8350, -8.3520, -6.6949, -6.4507, -6.0567, -5.7893, -5.6004,\n",
      "        -5.4415, -5.3974, -3.9379, -3.7875, -3.3515, -3.2190, -3.0092])\n",
      "Prune 2000 samples.\n",
      "Frequency of bin counts [(838, 1), (928, 1), (643, 1), (591, 1)]\n",
      "Skipping 0 empty bins in total 4 bins\n",
      "/home/automl/d2pruning/core/data/Coreset.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rst[sorted_index] = torch.tensor(budgets).type(torch.int)\n",
      "Using density sampling...\n",
      "Starting process for label   0 with 838 samples\n",
      "8380 connected nodes\n",
      "Starting process for label   1 with 928 samples\n",
      "9280 connected nodes\n",
      "Starting process for label   2 with 643 samples\n",
      "6430 connected nodes\n",
      "Starting process for label   3 with 591 samples\n",
      "5910 connected nodes\n",
      "06/28/2025 17:40:25 - INFO - __main__ - Pruned dataset from 5000 samples to 2500 samples, retaining 0.5 of the dataset.\n",
      "['label', 'title', 'description'] ['label', 'title', 'description']\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'title', 'description'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'title', 'description'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'title', 'description'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "Pruned 5000 samples in original train set to 2500\n",
      "/home/automl/d2pruning/train_nlp_explore.py:899: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  np.save(coreset_index_path, np.array(coreset_index))\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/automl/.cache/huggingface/hub/models--google--electra-small-discriminator/snapshots/fa8239aadc095e9164941d05878b98afe9b953c3/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 2500/2500 [00:00<00:00, 7249.11 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 7955.78 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8054.89 examples/s\n",
      "06/28/2025 17:40:26 - INFO - __main__ - Sample 1372 of the training set: {'input_ids': [101, 12175, 2000, 13210, 7315, 3084, 7541, 3413, 1999, 3515, 2487, 2086, 1006, 26665, 1007, 102, 26665, 1011, 2019, 12175, 2315, 2005, 1037, 8730, 2643, 1032, 1997, 2162, 2097, 2272, 2004, 2485, 2000, 3011, 2023, 2733, 2004, 2009, 2038, 2144, 1032, 11502, 2509, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}.\n",
      "06/28/2025 17:40:26 - INFO - __main__ - Sample 1165 of the training set: {'input_ids': [101, 9098, 5016, 7151, 1053, 2509, 3463, 15488, 23212, 2078, 1005, 102, 2414, 1006, 26665, 1007, 1011, 2329, 2137, 9098, 1996, 2088, 1005, 1055, 2117, 5221, 9907, 9338, 1010, 3786, 19939, 2015, 2007, 1037, 2410, 1012, 1019, 3867, 4125, 1999, 2353, 1011, 4284, 11372, 2004, 27999, 6202, 2013, 3304, 1998, 3607, 16396, 3010, 3471, 1998, 1996, 2844, 9044, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.\n",
      "06/28/2025 17:40:26 - INFO - __main__ - Sample 1861 of the training set: {'input_ids': [101, 2149, 5268, 4152, 2423, 2086, 1999, 4028, 1997, 8956, 3457, 102, 1037, 2149, 5268, 2001, 7331, 2000, 2423, 2086, 1999, 3827, 2005, 1996, 4028, 1997, 2019, 8956, 2120, 4932, 2386, 1999, 2089, 1010, 1996, 2149, 2510, 2056, 5095, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.\n",
      "06/28/2025 17:40:28 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Num examples = 2500\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Instantaneous batch size per device = 32\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 17:40:28 - INFO - __main__ -   Total optimization steps = 790\n",
      "  0%|                                                   | 0/790 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████▏                                     | 79/790 [00:07<00:58, 12.17it/s]06/28/2025 17:40:39 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.814}\n",
      "epoch 0: validation: {'accuracy': 0.814}\n",
      "06/28/2025 17:40:42 - INFO - __main__ - epoch 0: test: {'accuracy': 0.818}\n",
      "epoch 0: test: {'accuracy': 0.818}\n",
      " 20%|████████▏                                | 157/790 [00:20<00:50, 12.49it/s]06/28/2025 17:40:51 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.859}\n",
      "epoch 1: validation: {'accuracy': 0.859}\n",
      "06/28/2025 17:40:54 - INFO - __main__ - epoch 1: test: {'accuracy': 0.883}\n",
      "epoch 1: test: {'accuracy': 0.883}\n",
      " 30%|████████████▎                            | 237/790 [00:32<00:44, 12.33it/s]06/28/2025 17:41:04 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.863}\n",
      "epoch 2: validation: {'accuracy': 0.863}\n",
      "06/28/2025 17:41:06 - INFO - __main__ - epoch 2: test: {'accuracy': 0.885}\n",
      "epoch 2: test: {'accuracy': 0.885}\n",
      " 40%|████████████████▎                        | 315/790 [00:44<00:39, 11.97it/s]06/28/2025 17:41:16 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.866}\n",
      "epoch 3: validation: {'accuracy': 0.866}\n",
      "06/28/2025 17:41:19 - INFO - __main__ - epoch 3: test: {'accuracy': 0.889}\n",
      "epoch 3: test: {'accuracy': 0.889}\n",
      " 50%|████████████████████▌                    | 395/790 [00:57<00:31, 12.71it/s]06/28/2025 17:41:28 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.87}\n",
      "epoch 4: validation: {'accuracy': 0.87}\n",
      "06/28/2025 17:41:31 - INFO - __main__ - epoch 4: test: {'accuracy': 0.878}\n",
      "epoch 4: test: {'accuracy': 0.878}\n",
      " 60%|████████████████████████▌                | 473/790 [01:09<00:26, 12.17it/s]06/28/2025 17:41:41 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.865}\n",
      "epoch 5: validation: {'accuracy': 0.865}\n",
      " 70%|████████████████████████████▋            | 553/790 [01:19<00:19, 12.37it/s]06/28/2025 17:41:50 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.865}\n",
      "epoch 6: validation: {'accuracy': 0.865}\n",
      " 80%|████████████████████████████████▋        | 631/790 [01:28<00:13, 11.84it/s]06/28/2025 17:42:00 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.867}\n",
      "epoch 7: validation: {'accuracy': 0.867}\n",
      " 90%|████████████████████████████████████▉    | 711/790 [01:37<00:06, 12.27it/s]06/28/2025 17:42:09 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.872}\n",
      "epoch 8: validation: {'accuracy': 0.872}\n",
      "06/28/2025 17:42:12 - INFO - __main__ - epoch 8: test: {'accuracy': 0.883}\n",
      "epoch 8: test: {'accuracy': 0.883}\n",
      "100%|████████████████████████████████████████▉| 789/790 [01:50<00:00, 12.08it/s]06/28/2025 17:42:21 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.871}\n",
      "epoch 9: validation: {'accuracy': 0.871}\n",
      "100%|█████████████████████████████████████████| 790/790 [01:53<00:00,  6.97it/s]\n"
     ]
    }
   ],
   "source": [
    "CORESET_RATIO = 0.5\n",
    "N_NEIGHBOR = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"google/electra-small-discriminator\" \\\n",
    "    --output_dir ./data-model/agnews-6k//coreset-0.5 \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --coreset \\\n",
    "    --coreset-mode class \\\n",
    "    --budget-mode uniform \\\n",
    "    --sampling-mode graph \\\n",
    "    --data-score-path ./data-model/agnews-6k/all-data/data-score-goddawg.pickle \\\n",
    "    --feature-path ./data-model/agnews-6k//all-data/train-features.npy \\\n",
    "    --coreset-key forgetting \\\n",
    "    --coreset-ratio {CORESET_RATIO} \\\n",
    "    --mis-ratio 0.4 \\\n",
    "    --label-balanced \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k/val_index.npy \\\n",
    "    --n-neighbor {N_NEIGHBOR} \\\n",
    "    --gamma {GAMMA} \\\n",
    "    --graph-mode sum \\\n",
    "    --graph-sampling-mode weighted \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c8a8837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/home/automl/d2pruning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/28/2025 17:51:33 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "/home/automl/d2pruning/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json\n",
      "loading file merges.txt from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"FacebookAI/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors\n",
      "Some weights of the model checkpoint at FacebookAI/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 2500/2500 [00:00<00:00, 8378.58 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8769.81 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 9323.37 examples/s\n",
      "06/28/2025 17:51:42 - INFO - __main__ - Sample 456 of the training set: (456, {'input_ids': [0, 791, 4, 104, 4, 4787, 42723, 15674, 6449, 29, 39286, 4995, 13303, 36, 1251, 43, 2, 2, 1251, 111, 83, 121, 4, 104, 4, 9338, 15, 395, 37457, 29639, 37, 56, 303, 5, 685, 25492, 9, 39286, 11, 5, 37457, 5412, 219, 1844, 160, 13303, 480, 1271, 39, 6680, 7, 10, 9001, 61, 37457, 7333, 33396, 38897, 13, 11505, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "06/28/2025 17:51:42 - INFO - __main__ - Sample 102 of the training set: (102, {'input_ids': [0, 7199, 241, 9038, 160, 7, 3306, 386, 2, 2, 133, 18563, 3445, 1357, 5, 191, 378, 363, 19, 10, 291, 12, 1360, 872, 23, 37802, 2880, 6, 10, 819, 14, 630, 849, 3416, 131, 90, 492, 203, 1034, 13, 10, 11713, 191, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}).\n",
      "06/28/2025 17:51:42 - INFO - __main__ - Sample 1126 of the training set: (1126, {'input_ids': [0, 19739, 42263, 9722, 37749, 7093, 3407, 1643, 11, 31589, 13623, 36, 1251, 43, 2, 2, 1251, 111, 20, 3234, 7093, 3407, 21324, 333, 381, 3847, 37457, 1193, 9725, 24, 21, 45, 10, 1240, 1370, 19, 273, 18, 13662, 37457, 25764, 23, 292, 3622, 12309, 4492, 6, 53, 24, 67, 617, 37457, 36508, 1070, 5, 11843, 14385, 19485, 108, 8099, 559, 4181, 6, 8324, 281, 4989, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "06/28/2025 17:51:44 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Num examples = 2500\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 17:51:44 - INFO - __main__ -   Total optimization steps = 1570\n",
      "  0%|                                                  | 0/1570 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|████                                    | 157/1570 [00:22<03:15,  7.23it/s]06/28/2025 17:52:09 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.882}\n",
      "epoch 0: validation: {'accuracy': 0.882}\n",
      "06/28/2025 17:52:12 - INFO - __main__ - epoch 0: test: {'accuracy': 0.897}\n",
      "epoch 0: test: {'accuracy': 0.897}\n",
      " 20%|████████                                | 314/1570 [00:51<03:21,  6.23it/s]06/28/2025 17:52:38 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.865}\n",
      "epoch 1: validation: {'accuracy': 0.865}\n",
      " 30%|████████████                            | 471/1570 [01:17<02:39,  6.88it/s]06/28/2025 17:53:04 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.883}\n",
      "epoch 2: validation: {'accuracy': 0.883}\n",
      "06/28/2025 17:53:07 - INFO - __main__ - epoch 2: test: {'accuracy': 0.892}\n",
      "epoch 2: test: {'accuracy': 0.892}\n",
      " 40%|████████████████                        | 628/1570 [01:46<02:16,  6.91it/s]06/28/2025 17:53:33 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.88}\n",
      "epoch 3: validation: {'accuracy': 0.88}\n",
      " 50%|████████████████████                    | 785/1570 [02:12<01:42,  7.64it/s]06/28/2025 17:53:59 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.886}\n",
      "epoch 4: validation: {'accuracy': 0.886}\n",
      "06/28/2025 17:54:02 - INFO - __main__ - epoch 4: test: {'accuracy': 0.899}\n",
      "epoch 4: test: {'accuracy': 0.899}\n",
      " 60%|████████████████████████                | 942/1570 [02:41<01:27,  7.16it/s]06/28/2025 17:54:28 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.885}\n",
      "epoch 5: validation: {'accuracy': 0.885}\n",
      " 70%|███████████████████████████▎           | 1099/1570 [03:07<01:01,  7.70it/s]06/28/2025 17:54:54 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.886}\n",
      "epoch 6: validation: {'accuracy': 0.886}\n",
      "06/28/2025 17:54:57 - INFO - __main__ - epoch 6: test: {'accuracy': 0.904}\n",
      "epoch 6: test: {'accuracy': 0.904}\n",
      " 80%|███████████████████████████████▏       | 1256/1570 [03:36<00:42,  7.46it/s]06/28/2025 17:55:23 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.882}\n",
      "epoch 7: validation: {'accuracy': 0.882}\n",
      " 90%|███████████████████████████████████    | 1413/1570 [04:02<00:21,  7.14it/s]06/28/2025 17:55:49 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.882}\n",
      "epoch 8: validation: {'accuracy': 0.882}\n",
      "100%|███████████████████████████████████████| 1570/1570 [04:28<00:00,  7.18it/s]06/28/2025 17:56:15 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.881}\n",
      "epoch 9: validation: {'accuracy': 0.881}\n",
      "100%|███████████████████████████████████████| 1570/1570 [04:31<00:00,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"FacebookAI/roberta-base\" \\\n",
    "    --output_dir ./data-model/agnews-6k/electra/bert-50%/ \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k/val_index.npy \\\n",
    "    --train-index-path ./data-model/agnews-6k//coreset-0.5/coreset-goddawg.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fec2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"google/electra-small-discriminator\"\n",
    "\"FacebookAI/roberta-base\"\n",
    "\"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9fb3b",
   "metadata": {},
   "source": [
    "### Run on AGNews-5K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "331492c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eded826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./.venv/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading aiohttp-3.12.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.6.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [multidict]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [multidict]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [multidict]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [multidict]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [datasets]/15\u001b[0m [datasets]]ss]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 datasets-3.6.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.1 multiprocess-0.70.16 propcache-0.3.2 pyarrow-20.0.0 xxhash-3.5.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b6349d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google-bert/bert-base-uncased'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"google/electra-small-discriminator\"\n",
    "\"FacebookAI/roberta-base\"\n",
    "\"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9236ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'd2pruning/'\n",
      "/drive1/nammt/d2pruning\n",
      "06/28/2025 16:49:36 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Loading validation set from saved index\n",
      "Reduced training set from 6000 to 5000 for creating validation set\n",
      "/drive1/nammt/d2pruning/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"goddawg/agnews-6k\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/automl/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset: 100%|█| 5000/5000 [00:00<00:00, 8997.58 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8883.00 examples/s\n",
      "Running tokenizer on dataset: 100%|█| 1000/1000 [00:00<00:00, 8694.88 examples/s\n",
      "06/28/2025 16:49:45 - INFO - __main__ - Sample 912 of the training set: (912, {'input_ids': [101, 3956, 1005, 1055, 10666, 3210, 2039, 2490, 2005, 5622, 5283, 2094, 3789, 102, 6744, 1006, 26665, 1007, 1011, 5611, 3539, 2704, 16126, 10666, 8610, 2098, 2490, 2005, 9432, 1005, 1055, 5622, 5283, 2094, 2283, 3789, 2000, 3288, 1996, 4450, 2283, 2046, 1037, 2231, 2008, 2052, 3796, 10245, 3864, 1998, 6643, 3726, 1996, 2126, 2005, 1037, 14474, 10534, 2279, 2095, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}).\n",
      "06/28/2025 16:49:45 - INFO - __main__ - Sample 204 of the training set: (204, {'input_ids': [101, 13420, 6971, 2039, 2007, 25353, 14905, 2937, 102, 2044, 3773, 2210, 3112, 2007, 1996, 4431, 5617, 2009, 2580, 2005, 7513, 1001, 4464, 1025, 1055, 26381, 4132, 1010, 13420, 2097, 2085, 2147, 2007, 25353, 14905, 2937, 2000, 3443, 4431, 7248, 2005, 1017, 2290, 2398, 8454, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "06/28/2025 16:49:45 - INFO - __main__ - Sample 2253 of the training set: (2253, {'input_ids': [101, 4654, 3540, 22879, 5668, 7523, 2322, 12954, 28397, 1999, 5279, 1006, 9706, 1007, 102, 9706, 1011, 4654, 3540, 22879, 5668, 3603, 2322, 23880, 12954, 28397, 1999, 1996, 13253, 3089, 3148, 18128, 1999, 2530, 5279, 1010, 1996, 2231, 1005, 1055, 2473, 1997, 21387, 2056, 9857, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 3}).\n",
      "Downloading builder script: 4.20kB [00:00, 5.97MB/s]\n",
      "06/28/2025 16:49:48 - INFO - __main__ - ***** Running training *****\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Num examples = 5000\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Num Epochs = 10\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/28/2025 16:49:48 - INFO - __main__ -   Total optimization steps = 3130\n",
      "  0%|                                                  | 0/3130 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 10%|███▉                                    | 312/3130 [00:43<06:39,  7.05it/s]06/28/2025 16:50:35 - INFO - __main__ - epoch 0: validation: {'accuracy': 0.896}\n",
      "epoch 0: validation: {'accuracy': 0.896}\n",
      "06/28/2025 16:50:38 - INFO - __main__ - epoch 0: test: {'accuracy': 0.906}\n",
      "epoch 0: test: {'accuracy': 0.906}\n",
      " 20%|████████                                | 626/3130 [01:34<05:07,  8.13it/s]06/28/2025 16:51:25 - INFO - __main__ - epoch 1: validation: {'accuracy': 0.895}\n",
      "epoch 1: validation: {'accuracy': 0.895}\n",
      "06/28/2025 16:51:28 - INFO - __main__ - epoch 1: test: {'accuracy': 0.897}\n",
      "epoch 1: test: {'accuracy': 0.897}\n",
      " 30%|████████████                            | 939/3130 [02:25<04:57,  7.36it/s]06/28/2025 16:52:16 - INFO - __main__ - epoch 2: validation: {'accuracy': 0.904}\n",
      "epoch 2: validation: {'accuracy': 0.904}\n",
      "06/28/2025 16:52:19 - INFO - __main__ - epoch 2: test: {'accuracy': 0.903}\n",
      "epoch 2: test: {'accuracy': 0.903}\n",
      " 40%|███████████████▌                       | 1252/3130 [03:17<04:42,  6.64it/s]06/28/2025 16:53:08 - INFO - __main__ - epoch 3: validation: {'accuracy': 0.91}\n",
      "epoch 3: validation: {'accuracy': 0.91}\n",
      "06/28/2025 16:53:11 - INFO - __main__ - epoch 3: test: {'accuracy': 0.907}\n",
      "epoch 3: test: {'accuracy': 0.907}\n",
      " 50%|███████████████████▌                   | 1565/3130 [04:08<03:13,  8.08it/s]06/28/2025 16:53:59 - INFO - __main__ - epoch 4: validation: {'accuracy': 0.905}\n",
      "epoch 4: validation: {'accuracy': 0.905}\n",
      "06/28/2025 16:54:02 - INFO - __main__ - epoch 4: test: {'accuracy': 0.916}\n",
      "epoch 4: test: {'accuracy': 0.916}\n",
      " 60%|███████████████████████▍               | 1877/3130 [05:00<03:07,  6.67it/s]06/28/2025 16:54:51 - INFO - __main__ - epoch 5: validation: {'accuracy': 0.9}\n",
      "epoch 5: validation: {'accuracy': 0.9}\n",
      "06/28/2025 16:54:54 - INFO - __main__ - epoch 5: test: {'accuracy': 0.912}\n",
      "epoch 5: test: {'accuracy': 0.912}\n",
      " 70%|███████████████████████████▎           | 2191/3130 [05:51<02:15,  6.91it/s]06/28/2025 16:55:42 - INFO - __main__ - epoch 6: validation: {'accuracy': 0.907}\n",
      "epoch 6: validation: {'accuracy': 0.907}\n",
      "06/28/2025 16:55:45 - INFO - __main__ - epoch 6: test: {'accuracy': 0.918}\n",
      "epoch 6: test: {'accuracy': 0.918}\n",
      " 80%|███████████████████████████████▏       | 2504/3130 [06:43<01:17,  8.04it/s]06/28/2025 16:56:34 - INFO - __main__ - epoch 7: validation: {'accuracy': 0.907}\n",
      "epoch 7: validation: {'accuracy': 0.907}\n",
      "06/28/2025 16:56:37 - INFO - __main__ - epoch 7: test: {'accuracy': 0.918}\n",
      "epoch 7: test: {'accuracy': 0.918}\n",
      " 90%|███████████████████████████████████    | 2817/3130 [07:35<00:41,  7.53it/s]06/28/2025 16:57:26 - INFO - __main__ - epoch 8: validation: {'accuracy': 0.906}\n",
      "epoch 8: validation: {'accuracy': 0.906}\n",
      "06/28/2025 16:57:29 - INFO - __main__ - epoch 8: test: {'accuracy': 0.918}\n",
      "epoch 8: test: {'accuracy': 0.918}\n",
      "100%|███████████████████████████████████████| 3130/3130 [08:27<00:00,  7.98it/s]06/28/2025 16:58:18 - INFO - __main__ - epoch 9: validation: {'accuracy': 0.908}\n",
      "epoch 9: validation: {'accuracy': 0.908}\n",
      "06/28/2025 16:58:21 - INFO - __main__ - epoch 9: test: {'accuracy': 0.917}\n",
      "epoch 9: test: {'accuracy': 0.917}\n",
      "100%|███████████████████████████████████████| 3130/3130 [08:34<00:00,  6.08it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd d2pruning/\n",
    "!python train_nlp_explore.py \\\n",
    "    --task_name \"goddawg/agnews-6k\" \\\n",
    "    --model_name_or_path \"google-bert/bert-base-uncased\" \\\n",
    "    --output_dir ./data-model/agnews-6k/all-data \\\n",
    "    --do_train \\\n",
    "    --do_test \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/agnews-6k//val_index.npy \\\n",
    "    --seed 42 \\\n",
    "    --data_seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86738bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
