{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccf334d",
   "metadata": {},
   "source": [
    "imDB - 2k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6d88a",
   "metadata": {},
   "source": [
    "Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd4481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: dill in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (0.32.4)\n",
      "Requirement already satisfied: packaging in d:\\research\\d2pruning\\env\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in d:\\research\\d2pruning\\env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\research\\d2pruning\\env\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\research\\d2pruning\\env\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\research\\d2pruning\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "# Install evaluate library for load_metric function\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "919ad982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in d:\\research\\d2pruning\\env\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\research\\d2pruning\\env\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\research\\d2pruning\\env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\research\\d2pruning\\env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\research\\d2pruning\\env\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\research\\d2pruning\\env\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d712e44",
   "metadata": {},
   "source": [
    " First Train on Full IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0842f2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train_nlp.py \\\n",
    "    --task_name imdb \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --output_dir ./data-model/imdb/all-data \\\n",
    "    --do_train \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 512 \\\n",
    "    --val-index-path ./data-model/imdb/val_index.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59948c8",
   "metadata": {},
   "source": [
    "Step 2: Generate Importance Scores and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abfc5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/17/2025 09:10:09 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cpu\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Research\\d2pruning\\train_nlp.py\", line 1376, in <module>\n",
      "    main()\n",
      "  File \"d:\\Research\\d2pruning\\train_nlp.py\", line 621, in main\n",
      "    raw_datasets = preprocess_for_val(args, raw_datasets, val_size=1000)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Research\\d2pruning\\train_nlp.py\", line 187, in preprocess_for_val\n",
      "    if os.path.exists(args.val_index_path):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 19, in exists\n",
      "TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType\n"
     ]
    }
   ],
   "source": [
    "!python train_nlp.py \\\n",
    "    --task_name imdb \\\n",
    "    --model_name_or_path ./data-model/imdb/all-data \\\n",
    "    --output_dir ./data-model/imdb/all-data \\\n",
    "    --do_eval \\\n",
    "    --eval_train \\\n",
    "    --save_feature \\\n",
    "    --save_confidence \\\n",
    "    --save_importance_scores \\\n",
    "    --training_dynamics \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --max_length 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d5a7bf",
   "metadata": {},
   "source": [
    "Step 3: Select 2k Examples and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1449cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for 2k selection\n",
    "CORESET_RATIO = 0.1  # 2.5k out of 25k examples\n",
    "N_NEIGHBOR = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "!python train_nlp.py \\\n",
    "    --task_name imdb \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --output_dir ./data-model/imdb/coreset-2k \\\n",
    "    --do_train \\\n",
    "    --coreset \\\n",
    "    --coreset-mode class \\\n",
    "    --budget-mode uniform \\\n",
    "    --sampling-mode graph \\\n",
    "    --data-score-path ./data-model/imdb/all-data/data-score-imdb.pickle \\\n",
    "    --feature-path ./data-model/imdb/all-data/train-features.npy \\\n",
    "    --coreset-key forgetting \\\n",
    "    --coreset-ratio 0.08 \\\n",
    "    --mis-ratio 0.4 \\\n",
    "    --label-balanced \\\n",
    "    --n-neighbor 5 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 256 \\\n",
    "    --val-index-path ./data-model/imdb/val_index.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1cc05",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# COLA Dataset Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52691331",
   "metadata": {},
   "source": [
    "Step 1: Train on Full COLA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898c8990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Research\\d2pruning\\train_nlp.py\", line 1366, in <module>\n",
      "    main()\n",
      "  File \"d:\\Research\\d2pruning\\train_nlp.py\", line 530, in main\n",
      "    args = parse_args()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Research\\d2pruning\\train_nlp.py\", line 518, in parse_args\n",
      "    assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
      "AssertionError: `train_file` should be a csv or a json file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python train_nlp.py \\\n",
    "    --train_file ./data-model/cola/dataset/train \\\n",
    "    --validation_file ./data-model/cola/dataset/validation \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --output_dir ./data-model/cola/all-data \\\n",
    "    --do_train \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3179ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare COLA dataset from GLUE\n",
    "import os\n",
    "os.makedirs('./data-model/cola', exist_ok=True)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('glue', 'cola')\n",
    "dataset.save_to_disk('./data-model/cola/dataset')\n",
    "print(\"COLA dataset downloaded and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fed1b",
   "metadata": {},
   "source": [
    "Step 2: Generate Importance Scores and Features for COLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6322552",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_nlp.py \\\n",
    "    --train_file ./data-model/cola/dataset/train \\\n",
    "    --validation_file ./data-model/cola/dataset/validation \\\n",
    "    --model_name_or_path ./data-model/cola/all-data \\\n",
    "    --output_dir ./data-model/cola/all-data \\\n",
    "    --do_eval \\\n",
    "    --eval_train \\\n",
    "    --save_feature \\\n",
    "    --save_confidence \\\n",
    "    --save_importance_scores \\\n",
    "    --training_dynamics \\\n",
    "    --per_device_eval_batch_size 32 \\\n",
    "    --max_length 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d53a0",
   "metadata": {},
   "source": [
    "Step 3: Select Subset and Train COLA (0.5 ratio for smaller dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a06d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for COLA selection (smaller dataset, higher ratio)\n",
    "CORESET_RATIO = 0.5  # 50% of COLA training data\n",
    "N_NEIGHBOR = 5\n",
    "GAMMA = 0.1\n",
    "\n",
    "!python train_nlp.py \\\n",
    "    --task_name cola \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --output_dir ./data-model/cola/coreset-50pct \\\n",
    "    --do_train \\\n",
    "    --coreset \\\n",
    "    --coreset-mode class \\\n",
    "    --budget-mode uniform \\\n",
    "    --sampling-mode graph \\\n",
    "    --data-score-path ./data-model/cola/all-data/data-score-cola.pickle \\\n",
    "    --feature-path ./data-model/cola/all-data/train-features.npy \\\n",
    "    --coreset-key forgetting \\\n",
    "    --coreset-ratio 0.5 \\\n",
    "    --mis-ratio 0.4 \\\n",
    "    --label-balanced \\\n",
    "    --n-neighbor 5 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df822b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train on the downloaded COLA dataset\n",
    "!python train_nlp.py \\\n",
    "    --train_file ./data-model/cola/dataset/train \\\n",
    "    --validation_file ./data-model/cola/dataset/validation \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --output_dir ./data-model/cola/all-data \\\n",
    "    --do_train \\\n",
    "    --train_logger \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --max_length 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997935f6",
   "metadata": {},
   "source": [
    "Check and Update .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2c14ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ .gitignore exists and env/ is already ignored\n",
      "\n",
      "==================================================\n",
      "Current git status for env folder:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if .gitignore exists and contains env/\n",
    "import os\n",
    "\n",
    "gitignore_path = '.gitignore'\n",
    "env_ignored = False\n",
    "\n",
    "if os.path.exists(gitignore_path):\n",
    "    with open(gitignore_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        if 'env/' in content:\n",
    "            env_ignored = True\n",
    "            print(\"✅ .gitignore exists and env/ is already ignored\")\n",
    "        else:\n",
    "            print(\"⚠️ .gitignore exists but env/ is not ignored\")\n",
    "else:\n",
    "    print(\"❌ .gitignore does not exist\")\n",
    "\n",
    "if not env_ignored:\n",
    "    # Create or update .gitignore\n",
    "    gitignore_content = \"\"\"# Python virtual environment\n",
    "env/\n",
    "venv/\n",
    "ENV/\n",
    "env.bak/\n",
    "venv.bak/\n",
    "\n",
    "# Python cache files\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "\n",
    "# Model files and data\n",
    "data-model/\n",
    "*.pickle\n",
    "*.pkl\n",
    "*.npy\n",
    "*.h5\n",
    "*.pth\n",
    "*.bin\n",
    "\n",
    "# Jupyter Notebook\n",
    ".ipynb_checkpoints\n",
    "\n",
    "# IDE files\n",
    ".vscode/\n",
    ".idea/\n",
    "\n",
    "# OS files\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Log files\n",
    "*.log\n",
    "logs/\n",
    "\"\"\"\n",
    "    \n",
    "    with open(gitignore_path, 'w') as f:\n",
    "        f.write(gitignore_content)\n",
    "    \n",
    "    print(\"✅ .gitignore created/updated with env/ folder ignored\")\n",
    "\n",
    "# Double check git status\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Current git status for env folder:\")\n",
    "os.system('git status env/ 2>/dev/null || echo \"env/ is properly ignored or not tracked\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
